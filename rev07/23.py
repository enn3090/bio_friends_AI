"""
[OpenCode ë³€í™˜ë³¸] 
Closed LLM(OpenAI) â†’ ì˜¤í”ˆì†ŒìŠ¤ LLM(ë¡œì»¬ Ollama + DeepSeek-R1) + Gradio UI

- âœ… API Key ë¶ˆí•„ìš” (ë¡œì»¬ PCì— Ollama ë° ëª¨ë¸ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
- âœ… ì„ë² ë”©: OllamaEmbeddings (ì˜ˆ: nomic-embed-text)
- âœ… LLM: ChatOllama ("deepseek-r1:latest")
- âœ… ë²¡í„°DB: Chroma(persist)
- âœ… PDF ë¡œë”: PyPDFLoader
- âœ… í…ìŠ¤íŠ¸ ë¶„í• : RecursiveCharacterTextSplitter
- âœ… ê°„ë‹¨í•œ ë¼ìš°íŒ… + ê´€ë ¨ì„± ê·¸ë ˆì´ë” + RAG ìƒì„±
- âœ… Gradio ì¸í„°í˜ì´ìŠ¤ ì œê³µ

ì‚¬ì „ ì¤€ë¹„(í„°ë¯¸ë„):
1) Ollama ì„¤ì¹˜ í›„ ëª¨ë¸ ì¤€ë¹„
   - ollama pull deepseek-r1:latest
   - ollama pull nomic-embed-text

2) íŒŒì´ì¬ íŒ¨í‚¤ì§€
   - pip install langchain langchain-community langchain-text-splitters chromadb pypdf gradio

í´ë” êµ¬ì¡° ê°€ì •:
- ../data/*.pdf          : ìƒ‰ì¸í•  PDFë“¤
- ../chroma_store        : Chroma í¼ì‹œìŠ¤íŠ¸ ë””ë ‰í„°ë¦¬
"""

from __future__ import annotations

import os
from glob import glob
from typing import List, Dict, Any, Literal, Tuple

# LangChain - ë¡œì»¬(Community) ìƒíƒœê³„ êµ¬ì„±ìš”ì†Œ
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain.schema import Document
from langchain_chroma import Chroma

import gradio as gr


# =========================
# 1) PDF ì½ê¸° ë° ë¶„í•  í•¨ìˆ˜
# =========================
def read_pdf_and_split_text(pdf_path: str, chunk_size: int = 1000, chunk_overlap: int = 100) -> List[Document]:
    """
    ì£¼ì–´ì§„ PDF íŒŒì¼ì„ ì½ê³  í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        pdf_path (str): PDF íŒŒì¼ì˜ ê²½ë¡œ.
        chunk_size (int): ê° í…ìŠ¤íŠ¸ ì²­í¬ì˜ í¬ê¸°. ê¸°ë³¸ê°’ì€ 1000.
        chunk_overlap (int): ì²­í¬ ê°„ì˜ ì¤‘ì²© í¬ê¸°. ê¸°ë³¸ê°’ì€ 100.

    ë°˜í™˜ê°’:
        List[Document]: ë¶„í• ëœ LangChain Document ë¦¬ìŠ¤íŠ¸.
    """
    print(f"[PDF] {pdf_path} -----------------------------")
    pdf_loader = PyPDFLoader(pdf_path)
    data_from_pdf = pdf_loader.load()

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap
    )

    splits = text_splitter.split_documents(data_from_pdf)
    print(f" -> Number of splits: {len(splits)}\n")
    return splits


# ====================================
# 2) ì„ë² ë”©/ë²¡í„°DB(Chroma) ì´ˆê¸°í™”/ë¡œë”©
# ====================================
def build_or_load_vectorstore(
    data_glob: str = "../data/*.pdf",
    persist_directory: str = "../chroma_store",
    embedding_model: str = "nomic-embed-text",
) -> Chroma:
    """
    Chroma ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ë¡œë“œí•©ë‹ˆë‹¤.
    - OllamaEmbeddingsë¥¼ ì´ìš©í•´ ë¡œì»¬ ì„ë² ë”© ìˆ˜í–‰.
    - persist_directoryê°€ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ PDFë¥¼ ì„ë² ë”©í•´ ìƒì„±.

    ë°˜í™˜:
        Chroma ì¸ìŠ¤í„´ìŠ¤
    """
    # ğŸ”¹ Ollama ì„ë² ë”© (ì‚¬ì „: ollama pull nomic-embed-text)
    embedding = OllamaEmbeddings(model=embedding_model)

    if os.path.exists(persist_directory) and len(os.listdir(persist_directory)) > 0:
        print("[VectorStore] ê¸°ì¡´ Chroma store ë¡œë”©")
        vectorstore = Chroma(
            persist_directory=persist_directory,
            embedding_function=embedding,
        )
        return vectorstore

    print("[VectorStore] ì‹ ê·œ Chroma store ìƒì„±")
    all_docs: List[Document] = []
    pdf_files = sorted(glob(data_glob))
    if not pdf_files:
        print(f"âš ï¸ PDFë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {data_glob}")
    for path in pdf_files:
        chunks = read_pdf_and_split_text(path)
        # ë©”íƒ€ë°ì´í„° ë³´ê°•(íŒŒì¼ëª…)
        for d in chunks:
            d.metadata = {**d.metadata, "source_file": os.path.basename(path)}
        all_docs.extend(chunks)

    # 100ê°œì”© ë‚˜ëˆ ì„œ ì´ˆê¸° ìƒ‰ì¸
    vectorstore: Chroma | None = None
    for i in range(0, len(all_docs), 100):
        batch = all_docs[i : i + 100]
        if vectorstore is None:
            vectorstore = Chroma.from_documents(
                documents=batch,
                embedding=embedding,
                persist_directory=persist_directory,
            )
        else:
            vectorstore.add_documents(documents=batch)

    if vectorstore is None:
        # ë¹ˆ ìŠ¤í† ì–´ë¼ë„ ë°˜í™˜ (ì˜¤ë¥˜ ë°©ì§€)
        vectorstore = Chroma(
            persist_directory=persist_directory, embedding_function=embedding
        )

    print("[VectorStore] ì¤€ë¹„ ì™„ë£Œ")
    return vectorstore


# ===============================
# 3) LLM (DeepSeek-R1 via Ollama)
# ===============================
def build_llm(model_name: str = "deepseek-r1:latest") -> ChatOllama:
    """
    ë¡œì»¬ Ollamaì˜ DeepSeek-R1 ëª¨ë¸ì„ Chat ì¸í„°í˜ì´ìŠ¤ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    - ì‚¬ì „ ì¤€ë¹„: ollama pull deepseek-r1:latest
    - R1ì€ <think>...</think> ë‚´ë¶€ ì‚¬ìœ  í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
      ìµœì¢… ì¶œë ¥ì—ì„œëŠ” ì œê±°(í›„ì²˜ë¦¬)í•˜ëŠ” í¸ì´ ì¢‹ìŠµë‹ˆë‹¤.
    """
    llm = ChatOllama(
        model=model_name,
        temperature=0.3,  # ì¼ê´€ì„± ìœ„ì£¼
        num_ctx=8192,     # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì—¬ìœ 
    )
    return llm


def strip_think_tags(text: str) -> str:
    """
    DeepSeek-R1ì˜ <think>...</think> ë‚´ë¶€ ì‚¬ìœ  í…ìŠ¤íŠ¸ë¥¼ ì œê±°í•˜ì—¬ ê¹”ë”í•œ ë‹µë³€ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    """
    import re

    # ì—¬ëŸ¬ ë²ˆ ë“±ì¥í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì „ë¶€ ì œê±°
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL | re.IGNORECASE)
    # ì•ë’¤ ê³µë°± ì •ë¦¬
    return text.strip()


# =====================================
# 4) ë¼ìš°íŒ…/ê·¸ë ˆì´ë”©(ê°„ë‹¨ ê·œì¹™ + LLM ë³´ì¡°)
# =====================================
VECTORTOPICS_KW = [
    "ì„œìš¸", "ì„œìš¸ì‹œ", "ë‰´ìš•", "ììœ¨ì£¼í–‰", "ì˜¨ì‹¤ê°€ìŠ¤", "ë°œì „ê³„íš", "ë„ì‹œ", "ê³„íš", "ì €ê°", "êµí†µ", "í”Œëœ",
]


def simple_router(question: str) -> Literal["vectorstore", "casual_talk"]:
    """
    ê·œì¹™ ê¸°ë°˜ ë¼ìš°í„°:
    - ì§ˆë¬¸ ì•ˆì— ë„ì‹œê³„íš/ì •ì±… ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ë©´ vectorstore
    - ê·¸ ì™¸ëŠ” casual_talk
    """
    q = (question or "").lower()
    if any(kw in question for kw in VECTORTOPICS_KW):
        return "vectorstore"
    # ê¸¸ì´ê°€ ë§¤ìš° ì§§ê³  ì¸ì‚¬/ì•ˆë¶€/ì¡ë‹´ì´ë©´ casual
    smalltalk_kw = ["ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "í•˜ì´", "ì˜ ì§€ëƒˆ", "ìš”ì¦˜ ì–´ë•Œ", "ë­í•´", "ê³ ë§ˆì›Œ"]
    if any(kw in question for kw in smalltalk_kw):
        return "casual_talk"
    # ê¸°ë³¸ê°’
    return "vectorstore" if len(q) > 12 else "casual_talk"


def simple_relevance_grader(doc_text: str, question: str) -> bool:
    """
    ë§¤ìš° ë‹¨ìˆœí•œ ê´€ë ¨ì„± íŒë‹¨(í‚¤ì›Œë“œ ê²¹ì¹¨ ê¸°ë°˜).
    - ë¹ ë¥´ê³  ì˜ì¡´ì„± ì—†ì´ ë™ì‘.
    - í•„ìš”ì‹œ LLM ë³´ì¡° ê·¸ë ˆì´ë”ë¡œ í™•ì¥ ê°€ëŠ¥.
    """
    q = question.lower()
    d = doc_text.lower()
    hits = 0
    for kw in set([*VECTORTOPICS_KW, *q.split()]):
        if len(kw) < 2:
            continue
        if kw in d:
            hits += 1
    return hits >= 2  # ê²¹ì¹˜ëŠ” í† í°ì´ ì¼ì • ì´ìƒì´ë©´ ê´€ë ¨ ìˆë‹¤ê³  ê°„ì£¼


# ===============================
# 5) RAG ì²´ì¸(ê²€ìƒ‰ â†’ í•„í„° â†’ ìƒì„±)
# ===============================
def run_rag_pipeline(
    question: str,
    vectorstore: Chroma,
    llm: ChatOllama,
    k: int = 5,
    max_context_chars: int = 6000,
) -> Tuple[str, List[Dict[str, Any]]]:
    """
    - ë¼ìš°íŒ… â†’ (vectorstore ê²€ìƒ‰) â†’ ê·¸ë ˆì´ë”© â†’ LLM ìƒì„±
    - casual_talk ë¼ìš°íŠ¸ë©´ ë‹¨ìˆœ LLM ì‘ë‹µ

    ë°˜í™˜:
        (final_answer, sources)
        - final_answer: ìµœì¢… ë‹µë³€ ë¬¸ìì—´(think ì œê±°)
        - sources: ì‚¬ìš©ëœ ë¬¸ì„œ ë©”íƒ€/ë¯¸ë¦¬ë³´ê¸° ë¦¬ìŠ¤íŠ¸
    """
    route = simple_router(question)

    if route == "casual_talk":
        prompt = f"ë‹¤ìŒ ì§ˆë¬¸/ëŒ€í™”ì— í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê²Œ ë‹µí•˜ì„¸ìš”.\n\nì§ˆë¬¸: {question}"
        raw = llm.invoke(prompt).content
        return strip_think_tags(raw), []

    # vectorstore ê²½ë¡œ
    retriever = vectorstore.as_retriever(search_kwargs={"k": k})
    docs: List[Document] = retriever.invoke(question)

    # ê°„ì´ ê·¸ë ˆì´ë”©ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê±°ë¥´ê¸°
    filtered: List[Document] = []
    for d in docs:
        if simple_relevance_grader(d.page_content, question):
            filtered.append(d)

    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±(ê¸¸ì´ ì œí•œ)
    context_parts = []
    total_len = 0
    used_sources: List[Dict[str, Any]] = []
    for d in filtered:
        snippet = d.page_content.strip()
        meta = d.metadata or {}
        # ë„ˆë¬´ ê¸´ ì²­í¬ëŠ” ì¼ë¶€ë§Œ
        if len(snippet) > 1200:
            snippet = snippet[:1100] + " ..."

        addition = f"[source: {meta.get('source_file','unknown')} | page: {meta.get('page', 'NA')}]\n{snippet}\n"
        if total_len + len(addition) > max_context_chars:
            break
        context_parts.append(addition)
        total_len += len(addition)

        used_sources.append(
            {
                "source_file": meta.get("source_file", "unknown"),
                "page": meta.get("page", "NA"),
                "preview": snippet[:300],
            }
        )

    context = "\n\n".join(context_parts) if context_parts else "(ê´€ë ¨ ë¬¸ì„œê°€ ì¶©ë¶„ì¹˜ ì•ŠìŠµë‹ˆë‹¤.)"

    # ë‹µë³€ í”„ë¡¬í”„íŠ¸
    system = (
        "ë„ˆëŠ” ë„ì‹œ ê³„íš/ì •ì±… ì „ë¬¸ê°€ì•¼. ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•´ì„œ "
        "ì§ˆë¬¸ì— ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µí•´ì¤˜. "
        "ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì‚¬ì‹¤ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  ë§í•´."
    )
    user = f"""[ì§ˆë¬¸]
{question}

[ì»¨í…ìŠ¤íŠ¸]
{context}

[ì§€ì¹¨]
- í•„ìš”í•œ ê²½ìš° ì„œìš¸/ë‰´ìš•ì˜ ì •ì±… ë§¥ë½ì„ ê°„ë‹¨íˆ ì •ë¦¬.
- ê·¼ê±°ê°€ ëœ ì†ŒìŠ¤ íŒŒì¼ëª…ê³¼ í˜ì´ì§€ë¥¼ ë¬¸ì¥ ëì— (íŒŒì¼ëª… p.í˜ì´ì§€) í˜•íƒœë¡œ ê°€ë³ê²Œ í‘œê¸°.
- ëª¨ë¥´ëŠ” ë‚´ìš©ì€ 'ì œì‹œëœ ë¬¸ì„œì—ì„œ í™•ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'ë¼ê³  ëª…ì‹œ.
"""

    raw_answer = llm.invoke(f"{system}\n\n{user}").content
    final_answer = strip_think_tags(raw_answer)
    return final_answer, used_sources


# =========================
# 6) Gradio UI êµ¬ì„±
# =========================
# ì „ì—­ ë¦¬ì†ŒìŠ¤(ì•± ì‹œì‘ ì‹œ 1íšŒ ì¤€ë¹„)
VECTORSTORE = build_or_load_vectorstore(
    data_glob="../data/*.pdf",
    persist_directory="../chroma_store",
    embedding_model="nomic-embed-text",
)
LLM = build_llm("deepseek-r1:latest")


def ui_ask(question: str) -> Tuple[str, List[Dict[str, Any]]]:
    """
    Gradioìš© í•¸ë“¤ëŸ¬. ì§ˆë¬¸ì„ ë°›ì•„ RAG íŒŒì´í”„ë¼ì¸ ìˆ˜í–‰.
    """
    if not question or not question.strip():
        return "ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.", []

    answer, sources = run_rag_pipeline(
        question=question.strip(), vectorstore=VECTORSTORE, llm=LLM, k=5
    )
    return answer, sources


# Gradio Blocks
with gr.Blocks(title="OpenCode â€¢ ë¡œì»¬ DeepSeek-R1 RAG") as demo:
    gr.Markdown(
        """
# OpenCode â€¢ ë¡œì»¬ DeepSeek-R1 RAG (Ollama + Chroma)
- ì¢Œì¸¡ ì…ë ¥ì— ì§ˆë¬¸ì„ ì ê³  **ì§ˆì˜í•˜ê¸°**ë¥¼ ëˆ„ë¥´ì„¸ìš”.
- ì£¼ì œì— ë”°ë¼ ìë™ìœ¼ë¡œ ìƒ‰ì¸ëœ PDF(VectorStore) ê²€ìƒ‰ ë˜ëŠ” ì¼ìƒ ëŒ€í™”ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.
- API Key ë¶ˆí•„ìš”. ëª¨ë“  ì—°ì‚°ì€ ë¡œì»¬ì—ì„œ ìˆ˜í–‰ë©ë‹ˆë‹¤.
        """
    )

    with gr.Row():
        with gr.Column(scale=3):
            inp = gr.Textbox(
                label="ì§ˆë¬¸ (ì˜ˆ: ì„œìš¸ ì˜¨ì‹¤ê°€ìŠ¤ ì €ê° ê³„íšì€ ë¬´ì—‡ì¸ê°€ìš”?)",
                placeholder="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
                lines=3,
            )
            btn = gr.Button("ì§ˆì˜í•˜ê¸°", variant="primary")

        with gr.Column(scale=5):
            out_md = gr.Markdown(label="ì‘ë‹µ")
            out_json = gr.JSON(label="ì°¸ì¡°ëœ ì†ŒìŠ¤(íŒŒì¼/í˜ì´ì§€/ë¯¸ë¦¬ë³´ê¸°)")

    btn.click(fn=ui_ask, inputs=inp, outputs=[out_md, out_json])

# ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§ì ‘ ì‹¤í–‰í•  ê²½ìš° Gradio ì•±ì„ ë„ì›ë‹ˆë‹¤.
if __name__ == "__main__":
    # share=Trueê°€ í•„ìš”í•˜ë©´ ì™¸ë¶€ ë…¸ì¶œ ê°€ëŠ¥(ì„ íƒ)
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)

