# -*- coding: utf-8 -*-
"""
[OpenCode ë³€í™˜ë³¸: Closed LLM(OpenAI) â†’ ì˜¤í”ˆì†ŒìŠ¤ LLM(Ollama â€¢ DeepSeek-R1) + Gradio GUI]

ë³¸ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì›ë³¸ ì½”ë“œì—ì„œ `ChatOpenAI`ì— ì˜ì¡´í•˜ë˜ ë¶€ë¶„ì„
ë¡œì»¬ì—ì„œ êµ¬ë™ë˜ëŠ” **Ollama + DeepSeek-R1** ëª¨ë¸ë¡œ ëŒ€ì²´í•˜ê³ ,
í„°ë¯¸ë„ ë£¨í”„ ëŒ€ì‹  **Gradio** ê¸°ë°˜ì˜ ê°„ë‹¨í•œ ëŒ€í™”í˜• UIë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì „ì œ:
- ë¡œì»¬ PCì— Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆê³ , deepseek-r1 ê³„ì—´ ëª¨ë¸ì´ pull ë˜ì–´ ìˆìŒ
  ì˜ˆ) `ollama pull deepseek-r1:7b` í˜¹ì€ `ollama pull deepseek-r1:32b`
- `utils.py`ì— `save_state` í•¨ìˆ˜ê°€ ì¡´ì¬í•œë‹¤ê³  ê°€ì •
- API Key ë¶ˆí•„ìš” (sk-... ì „í˜€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)

ì„¤ì¹˜:
    pip install langchain langgraph gradio
    # í™˜ê²½ì— ë”°ë¼
    pip install langchain-ollama  # ê¶Œì¥
    # ë˜ëŠ”
    pip install langchain-community

ì‹¤í–‰:
    python app.py
"""

from __future__ import annotations

import os
from datetime import datetime
from typing import List

import gradio as gr
from typing_extensions import TypedDict

from langgraph.graph import StateGraph, START, END

# LangChain ë©”ì‹œì§€/í”„ë¡¬í”„íŠ¸/íŒŒì„œ
from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage
from langchain_core.prompts import PromptTemplate

# Ollama(ë¡œì»¬ LLM) â€” íŒ¨í‚¤ì§€ ë²„ì „ì— ë”°ë¼ import ê²½ë¡œê°€ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ try/except ì²˜ë¦¬
try:
    from langchain_ollama import ChatOllama  # ìµœì‹  ê¶Œì¥
except Exception:
    try:
        from langchain_community.chat_models import ChatOllama  # ëŒ€ì²´ ê²½ë¡œ
    except Exception as e:
        raise ImportError(
            "ChatOllama ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `pip install langchain-ollama` "
            "ë˜ëŠ” `pip install langchain-community`ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”."
        )

# utils.py ì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
from utils import save_state


# =========================
# ê²½ë¡œ/íŒŒì¼ ì •ë³´
# =========================
try:
    filename = os.path.basename(__file__)  # í˜„ì¬ íŒŒì¼ëª…
    absolute_path = os.path.abspath(__file__)
    current_path = os.path.dirname(absolute_path)
except NameError:
    # ë…¸íŠ¸ë¶/REPL ë“±ì—ì„œ __file__ ì´ ì—†ì„ ê²½ìš° ëŒ€ë¹„
    filename = "app.py"
    absolute_path = os.path.abspath("./app.py")
    current_path = os.path.dirname(absolute_path)


# =========================
# ë¡œì»¬ LLM ì´ˆê¸°í™” (DeepSeek-R1 on Ollama)
# =========================
# í•„ìš” ì‹œ model íƒœê·¸ë¥¼ í™˜ê²½ì— ë§ê²Œ ë³€ê²½í•˜ì„¸ìš”. (ì˜ˆ: "deepseek-r1:32b")
llm = ChatOllama(
    model="deepseek-r1:7b",
    temperature=0.7,
    # ì•„ë˜ íŒŒë¼ë¯¸í„°ëŠ” Ollama ë° LangChain ë²„ì „ì— ë”°ë¼ ì§€ì› ì—¬ë¶€ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # num_ctx=4096,
    # top_p=0.9,
)


# =========================
# ìƒíƒœ ì •ì˜
# =========================
class State(TypedDict):
    messages: List[AnyMessage | str]


# =========================
# ë…¸ë“œ(ì—ì´ì „íŠ¸): communicator
# =========================
def communicator(state: State):
    """
    ì‚¬ìš©ìì˜ ë©”ì‹œì§€ ì´ë ¥(State.messages)ì„ ë°”íƒ•ìœ¼ë¡œ
    'ì»¤ë®¤ë‹ˆì¼€ì´í„°' ì—­í• ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print("\n\n============ COMMUNICATOR ============")

    communicator_system_prompt = PromptTemplate.from_template(
        """
        ë„ˆëŠ” ì±…ì„ ì“°ëŠ” AIíŒ€ì˜ ì»¤ë®¤ë‹ˆì¼€ì´í„°ë¡œì„œ, 
        AIíŒ€ì˜ ì§„í–‰ìƒí™©ì„ ì‚¬ìš©ìì—ê²Œ ë³´ê³ í•˜ê³ , ì‚¬ìš©ìì˜ ì˜ê²¬ì„ íŒŒì•…í•˜ê¸° ìœ„í•œ ëŒ€í™”ë¥¼ ë‚˜ëˆˆë‹¤. 

        messages: {messages}
        """
    )

    # LangChain Expression Language: í”„ë¡¬í”„íŠ¸ -> LLM
    system_chain = communicator_system_prompt | llm

    messages = state["messages"]
    inputs = {"messages": messages}

    # ìŠ¤íŠ¸ë¦¬ë° ìˆ˜ì‹  (ì½˜ì†”ì—ë„ ì‹¤ì‹œê°„ ì¶œë ¥)
    print('\nAI\t: ', end='')
    gathered_msg = None
    for chunk in system_chain.stream(inputs):
        # chunk ëŠ” AIMessage í˜¹ì€ í•´ë‹¹ Chunks ì¼ ìˆ˜ ìˆìŒ
        text = getattr(chunk, "content", "") if chunk else ""
        print(text, end='')
        if gathered_msg is None:
            gathered_msg = AIMessage(content=text)
        else:
            gathered_msg.content += text

    # ëŒ€í™” ì´ë ¥ì— AI ì‘ë‹µ ì¶”ê°€
    if gathered_msg is None:
        gathered_msg = AIMessage(content="(ì‘ë‹µ ìƒì„± ì‹¤íŒ¨)")
    messages.append(gathered_msg)

    return {"messages": messages}


# =========================
# ê·¸ë˜í”„ êµ¬ì„±
# =========================
graph_builder = StateGraph(State)
graph_builder.add_node("communicator", communicator)
graph_builder.add_edge(START, "communicator")
graph_builder.add_edge("communicator", END)
graph = graph_builder.compile()

# ê·¸ë˜í”„ ì‹œê°í™” (ì„ íƒ)
try:
    graph.get_graph().draw_mermaid_png(output_file_path=absolute_path.replace('.py', '.png'))
except Exception:
    # graphviz ë¯¸ì„¤ì¹˜ ë“±ìœ¼ë¡œ ì¸í•œ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
    pass


# =========================
# ì´ˆê¸° State ìƒì„±
# =========================
def initial_state() -> State:
    return State(
        messages=[
            SystemMessage(
                content=f"""
            ë„ˆí¬ AIë“¤ì€ ì‚¬ìš©ìì˜ ìš”êµ¬ì— ë§ëŠ” ì±…ì„ ì“°ëŠ” ì‘ê°€íŒ€ì´ë‹¤.
            ì‚¬ìš©ìê°€ ì‚¬ìš©í•˜ëŠ” ì–¸ì–´ë¡œ ëŒ€í™”í•˜ë¼.

            í˜„ì¬ì‹œê°ì€ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}ì´ë‹¤.
            """
            )
        ]
    )


# =========================
# Gradio UI
# =========================
"""
êµ¬ì„±:
- ì¢Œì¸¡: Chatbot (ëŒ€í™” ê¸°ë¡)
- í•˜ë‹¨: ì‚¬ìš©ì ì…ë ¥ì°½ + ì „ì†¡ ë²„íŠ¼
- ìš°ì¸¡: ìƒíƒœ/ì»¨íŠ¸ë¡¤ (ë©”ì‹œì§€ ìˆ˜, ì‘ì—… í´ë” í‘œì‹œ, ì´ˆê¸°í™” ë²„íŠ¼)

ë™ì‘:
1) ì‚¬ìš©ìê°€ ì…ë ¥ -> HumanMessage ë¡œ ì¶”ê°€
2) LangGraph íŒŒì´í”„ë¼ì¸(graph.invoke) ì‹¤í–‰
3) communicator ë…¸ë“œì˜ ìµœì‹  ì‘ë‹µì„ Chatbotì— í‘œì‹œ
4) state ì €ì¥ (utils.save_state)
"""

with gr.Blocks(title="OpenCode â€¢ DeepSeek-R1 (Ollama) â€¢ LangGraph + Gradio", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ğŸ“š OpenCode: LangGraph + DeepSeek-R1 (Ollama) + Gradio
        - **Closed LLM â†’ ì˜¤í”ˆì†ŒìŠ¤ LLM ë³€í™˜ë³¸**
        - ë¡œì»¬ **Ollama** + **DeepSeek-R1** ëª¨ë¸ ì‚¬ìš© (API Key ë¶ˆí•„ìš”)
        - ë‹¨ì¼ ë…¸ë“œ(**communicator**)ë¡œ ê°„ë‹¨ ëŒ€í™” íŒŒì´í”„ë¼ì¸
        """
    )

    with gr.Row():
        with gr.Column(scale=3):
            chat = gr.Chatbot(height=520, label="ëŒ€í™”ì°½")
            user_in = gr.Textbox(placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...", label="User ì…ë ¥")
            send_btn = gr.Button("ì „ì†¡", variant="primary")

        with gr.Column(scale=2):
            gr.Markdown(
                f"**ì‘ì—… í´ë”**: `{current_path}`\n\n"
                " - stateëŠ” `_runs/state.txt`ì— ì €ì¥ë©ë‹ˆë‹¤.\n"
            )
            msg_count = gr.Number(value=1, precision=0, label="ë©”ì‹œì§€ ê°œìˆ˜", interactive=False)
            reset_btn = gr.Button("ì„¸ì…˜ ì´ˆê¸°í™”", variant="secondary")

    st = gr.State(initial_state())

    def on_reset():
        s = initial_state()
        return s, [], 1

    reset_btn.click(on_reset, inputs=None, outputs=[st, chat, msg_count])

    def on_send(user_text: str, s: State, history: list[list[str, str]]):
        """
        Gradio í•¸ë“¤ëŸ¬:
        - user_text ë¥¼ HumanMessage ë¡œ ì¶”ê°€
        - LangGraph ì‹¤í–‰
        - ìµœì‹  AI ì‘ë‹µì„ Chatbot íˆìŠ¤í† ë¦¬ì— ë°˜ì˜
        - state ì €ì¥
        """
        user_text = (user_text or "").strip()
        if not user_text:
            return gr.update(), s, history, len(s["messages"])

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        s["messages"].append(HumanMessage(user_text))
        history = history + [[user_text, None]]

        # LangGraph íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        new_state = graph.invoke(s)

        # ìµœì‹  AI ì‘ë‹µ ì¶”ì¶œ
        ai_text = ""
        for m in reversed(new_state["messages"]):
            if isinstance(m, AIMessage):
                ai_text = m.content
                break

        # Chatbot ìµœì‹  í„´ ì±„ìš°ê¸°
        if history and history[-1][1] is None:
            history[-1][1] = ai_text or "(ì‘ë‹µ ì—†ìŒ)"

        # ìƒíƒœ ì €ì¥
        try:
            save_state(current_path, new_state)
        except Exception:
            pass

        return "", new_state, history, len(new_state["messages"])

    send_btn.click(
        on_send,
        inputs=[user_in, st, chat],
        outputs=[user_in, st, chat, msg_count],
    )
    user_in.submit(
        on_send,
        inputs=[user_in, st, chat],
        outputs=[user_in, st, chat, msg_count],
    )


# =========================
# CLI ëª¨ë“œ(ì˜µì…˜) â€” í•„ìš” ì‹œ ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©
# =========================
# def run_cli():
#     state = initial_state()
#     while True:
#         user_input = input("\nUser\t: ").strip()
#         if user_input.lower() in ['exit', 'quit', 'q']:
#             print("Goodbye!")
#             break
#         state["messages"].append(HumanMessage(user_input))
#         state = graph.invoke(state)
#         # ë§ˆì§€ë§‰ AI ì‘ë‹µ ì¶œë ¥
#         for m in reversed(state["messages"]):
#             if isinstance(m, AIMessage):
#                 print("\nAI\t:", m.content)
#                 break
#         print('\n------------------------------------ MESSAGE COUNT\t', len(state["messages"]))
#         save_state(current_path, state)

if __name__ == "__main__":
    # Gradio ì„œë²„ ì‹¤í–‰ (ê¸°ë³¸ ë¡œì»¬ë§Œ; ì™¸ë¶€ ì ‘ê·¼ í•„ìš” ì‹œ share=True)
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)