# -*- coding: utf-8 -*-
"""
[OpenCode ë³€í™˜ë³¸]

- ëª©ì : Closed LLM(OpenAI ChatGPT) ì˜ì¡´ ì½”ë“œë¥¼ *ë¡œì»¬ ì˜¤í”ˆì†ŒìŠ¤ LLM* (Ollama + DeepSeek-R1) ê¸°ë°˜ìœ¼ë¡œ
        ë™ì‘í•˜ë„ë¡ ë³€í™˜í•˜ê³ , **Gradio** ê¸°ë°˜ì˜ ê°„ë‹¨í•œ ê·¸ë˜í”½ ì¸í„°í˜ì´ìŠ¤ê¹Œì§€ ì œê³µí•©ë‹ˆë‹¤.

- ì „ì œ:
  1) ë¡œì»¬ PCì— Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆê³ , deepseek-r1 ê³„ì—´ ëª¨ë¸ì´ ë¡œì»¬ì— ì¡´ì¬í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
     (ì˜ˆ: `ollama pull deepseek-r1:7b` ë˜ëŠ” `ollama pull deepseek-r1:32b`)
  2) ê¸°ì¡´ ìœ í‹¸ í•¨ìˆ˜(utils.py)ë¡œ `save_state`, `get_outline`, `save_outline` ê°€ ì œê³µëœë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
  3) ë³¸ ì½”ë“œëŠ” API Keyê°€ ì „í˜€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (sk-... ì œê±°)

- ì‹¤í–‰ ì „ ì¤€ë¹„:
  pip install langchain langgraph langchain-ollama gradio
  ë˜ëŠ” (í™˜ê²½ì— ë”°ë¼) pip install langchain langgraph langchain-community gradio

- ì‹¤í–‰:
  python app.py  (íŒŒì¼ëª…ì„ app.pyë¼ê³  ê°€ì •)
"""

from __future__ import annotations

import os
from datetime import datetime
from typing import List

# LangGraph / LangChain
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict

# ë©”ì‹œì§€ íƒ€ì… (LangChain Core)
from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers.string import StrOutputParser

# Ollama(ë¡œì»¬) ê¸°ë°˜ ì±— ëª¨ë¸
# ìµœì‹  LangChainì—ì„œëŠ” `langchain_ollama` íŒ¨í‚¤ì§€ì˜ ChatOllama ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
# í™˜ê²½ì— ë”°ë¼ `langchain_community.chat_models` ì˜ ChatOllama ë¥¼ ì‚¬ìš©í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
try:
    from langchain_ollama import ChatOllama  # ìµœì‹ 
except Exception:
    try:
        from langchain_community.chat_models import ChatOllama  # êµ¬ë²„ì „/ëŒ€ì²´
    except Exception as e:
        raise ImportError(
            "ChatOllama ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `pip install langchain-ollama` "
            "ë˜ëŠ” `pip install langchain-community`ë¥¼ í™•ì¸í•˜ì„¸ìš”."
        )

# Gradio GUI
import gradio as gr

# ìœ í‹¸ (ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸°) â€” í”„ë¡œì íŠ¸ì— í¬í•¨ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
try:
    from utils import save_state, get_outline, save_outline
except Exception:
    # utils ë¶€ì¬ ì‹œì—ë„ ë°ëª¨ ë™ì‘ ê°€ëŠ¥í•˜ë„ë¡ ê°„ë‹¨í•œ ëŒ€ì²´ êµ¬í˜„ ì œê³µ
    def save_state(path: str, state_obj: dict):
        os.makedirs(os.path.join(path, "_runs"), exist_ok=True)
        with open(os.path.join(path, "_runs", "state.txt"), "w", encoding="utf-8") as f:
            f.write(str(state_obj))

    def get_outline(path: str) -> str:
        fpath = os.path.join(path, "_runs", "outline.md")
        if os.path.exists(fpath):
            return open(fpath, "r", encoding="utf-8").read()
        return ""

    def save_outline(path: str, content: str):
        os.makedirs(os.path.join(path, "_runs"), exist_ok=True)
        with open(os.path.join(path, "_runs", "outline.md"), "w", encoding="utf-8") as f:
            f.write(content)


# =========================
# ê²½ë¡œ/íŒŒì¼ ì •ë³´
# =========================
try:
    filename = os.path.basename(__file__)
    absolute_path = os.path.abspath(__file__)
    current_path = os.path.dirname(absolute_path)
except NameError:
    # ë…¸íŠ¸ë¶/REPL ë“±ì—ì„œ __file__ ì´ ì—†ì„ ê²½ìš° ëŒ€ë¹„
    filename = "app.py"
    absolute_path = os.path.abspath("./app.py")
    current_path = os.path.dirname(absolute_path)


# =========================
# ë¡œì»¬ LLM(DeepSeek-R1 on Ollama) ì´ˆê¸°í™”
# =========================
# - ëª¨ë¸ ì´ë¦„ì€ ë¡œì»¬ ì„¤ì¹˜ëœ íƒœê·¸ì— ë§ê²Œ ë³€ê²½ ê°€ëŠ¥:
#   ì˜ˆ: "deepseek-r1:7b", "deepseek-r1:32b"
# - stream ì§€ì› ë° í•©ë¦¬ì ì¸ íŒŒë¼ë¯¸í„° ê¸°ë³¸ê°’ ì§€ì •
llm = ChatOllama(
    model="deepseek-r1:7b",  # í•„ìš” ì‹œ "deepseek-r1:32b" ë“±ìœ¼ë¡œ êµì²´
    temperature=0.7,
    # Ollama ìƒì„± íŒŒë¼ë¯¸í„° ì˜ˆì‹œ (í™˜ê²½/ë²„ì „ì— ë”°ë¼ ì§€ì› í‚¤ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤)
    # num_ctx=4096,
    # top_p=0.9,
)


# =========================
# ìƒíƒœ ì •ì˜ (LangGraph)
# =========================
class State(TypedDict):
    messages: List[AnyMessage | str]


# =========================
# ë…¸ë“œ(ì—ì´ì „íŠ¸) ì •ì˜
# =========================
def content_strategist(state: State):
    """
    [ì½˜í…ì¸  ì „ëµê°€]
    - ì´ì „ ëŒ€í™”ì™€ ì €ì¥ëœ ëª©ì°¨(outline)ë¥¼ í† ëŒ€ë¡œ ìƒˆë¡œìš´/ìˆ˜ì •ëœ ì„¸ë¶€ ëª©ì°¨ë¥¼ ì œì•ˆ.
    - ì œì•ˆëœ ëª©ì°¨ëŠ” íŒŒì¼ë¡œ ì €ì¥(save_outline).
    - ì‚¬ìš©ìì—ê²ŒëŠ” "ëª©ì°¨ ì‘ì„± ì™„ë£Œ"ë¼ëŠ” ë©”íƒ€ ë©”ì‹œì§€ë§Œ ì „ë‹¬(ëª©ì°¨ ìì²´ëŠ” ì»¤ë®¤ë‹ˆì¼€ì´í„°ê°€ ë°˜ë³µ ì¶œë ¥í•˜ì§€ ì•Šë„ë¡).
    """
    print("\n\n============ CONTENT STRATEGIST ============")

    content_strategist_system_prompt = PromptTemplate.from_template(
        """
        ë„ˆëŠ” ì±…ì„ ì“°ëŠ” AIíŒ€ì˜ ì½˜í…ì¸  ì „ëµê°€(Content Strategist)ë¡œì„œ,
        ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ê³ , AIíŒ€ì´ ì“¸ ì±…ì˜ ì„¸ë¶€ ëª©ì°¨ë¥¼ ê²°ì •í•œë‹¤.

        ì§€ë‚œ ëª©ì°¨ê°€ ìˆë‹¤ë©´ ê·¸ ë²„ì „ì„ ì‚¬ìš©ìì˜ ìš”êµ¬ì— ë§ê²Œ ìˆ˜ì •í•˜ê³ , ì—†ë‹¤ë©´ ìƒˆë¡œìš´ ëª©ì°¨ë¥¼ ì œì•ˆí•œë‹¤.

        --------------------------------
        - ì§€ë‚œ ëª©ì°¨: {outline}
        --------------------------------
        - ì´ì „ ëŒ€í™” ë‚´ìš©: {messages}
        """
    )

    # LangChain Expression Language(LCEL): í”„ë¡¬í”„íŠ¸ -> LLM -> ë¬¸ìì—´ íŒŒì„œ
    chain = content_strategist_system_prompt | llm | StrOutputParser()

    messages = state["messages"]
    outline = get_outline(current_path)

    inputs = {"messages": messages, "outline": outline}

    # ìŠ¤íŠ¸ë¦¬ë° ìˆ˜ì§‘ (ì½˜ì†”ì—ë„ ì‹¤ì‹œê°„ ì¶œë ¥)
    gathered = ""
    for chunk in chain.stream(inputs):
        gathered += chunk
        print(chunk, end="")
    print()

    # ëª©ì°¨ ì €ì¥
    save_outline(current_path, gathered)

    # ìƒíƒœ ë©”ì‹œì§€ì— "ì™„ë£Œ" ë©”íƒ€ ì •ë³´ë§Œ ì¶”ê°€
    done_msg = "[Content Strategist] ëª©ì°¨ ì‘ì„± ì™„ë£Œ"
    print(done_msg)
    messages.append(AIMessage(content=done_msg))

    return {"messages": messages}


def communicator(state: State):
    """
    [ì»¤ë®¤ë‹ˆì¼€ì´í„°]
    - íŒ€ì˜ ì§„í–‰ìƒí™©ì„ ì‚¬ìš©ìì—ê²Œ ë³´ê³ í•˜ê³ , í”¼ë“œë°±ì„ ìˆ˜ì§‘í•œë‹¤.
    - ì‚¬ìš©ìëŠ” ì´ë¯¸ outline íŒŒì¼ì„ ë³´ê³  ìˆë‹¤ê³  ê°€ì •í•˜ë¯€ë¡œ, ì—¬ê¸°ì„œ ëª©ì°¨ ì „ë¬¸ì„ ì¬ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.
    """
    print("\n\n============ COMMUNICATOR ============")

    communicator_system_prompt = PromptTemplate.from_template(
        """
        ë„ˆëŠ” ì±…ì„ ì“°ëŠ” AIíŒ€ì˜ ì»¤ë®¤ë‹ˆì¼€ì´í„°ë¡œì„œ, 
        AIíŒ€ì˜ ì§„í–‰ìƒí™©ì„ ì‚¬ìš©ìì—ê²Œ ë³´ê³ í•˜ê³ , ì‚¬ìš©ìì˜ ì˜ê²¬ì„ íŒŒì•…í•˜ê¸° ìœ„í•œ ëŒ€í™”ë¥¼ ë‚˜ëˆˆë‹¤. 

        ì‚¬ìš©ìë„ outline(ëª©ì°¨)ì„ ì´ë¯¸ ë³´ê³  ìˆìœ¼ë¯€ë¡œ, ë‹¤ì‹œ ì¶œë ¥í•  í•„ìš”ëŠ” ì—†ë‹¤.

        messages: {messages}
        """
    )

    chain = communicator_system_prompt | llm

    messages = state["messages"]
    inputs = {"messages": messages}

    # ìŠ¤íŠ¸ë¦¼ ìˆ˜ì‹  (LLMì´ í† í° ë‹¨ìœ„ë¡œ ìƒì„±í•  ê²½ìš°)
    gathered_msg = None
    print("\nAI\t: ", end="")
    for chunk in chain.stream(inputs):
        # chunk ëŠ” ChatMessage íƒ€ì…ì— ì¤€í•˜ëŠ” í† ë§‰
        print(chunk.content, end="")
        if gathered_msg is None:
            gathered_msg = chunk
        else:
            # LangChainì˜ ë©”ì‹œì§€ í•©ì„± ì—°ì‚°ìëŠ” ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ì•ˆì „í•˜ê²Œ ì´ì–´ë¶™ì„
            gathered_msg = type(chunk)(
                content=(gathered_msg.content + chunk.content),
                additional_kwargs=getattr(chunk, "additional_kwargs", {}),
                response_metadata=getattr(chunk, "response_metadata", {}),
                id=getattr(chunk, "id", None),
                name=getattr(chunk, "name", None),
                example=getattr(chunk, "example", False),
                tool_calls=getattr(chunk, "tool_calls", None),
                invalid_tool_calls=getattr(chunk, "invalid_tool_calls", None),
            )

    messages.append(gathered_msg)
    return {"messages": messages}


# =========================
# ìƒíƒœ ê·¸ë˜í”„ êµ¬ì„±
# =========================
graph_builder = StateGraph(State)
graph_builder.add_node("content_strategist", content_strategist)
graph_builder.add_node("communicator", communicator)

graph_builder.add_edge(START, "content_strategist")
graph_builder.add_edge("content_strategist", "communicator")
graph_builder.add_edge("communicator", END)

graph = graph_builder.compile()

# ê·¸ë˜í”„ ì‹œê°í™” (ì˜µì…˜)
try:
    graph.get_graph().draw_mermaid_png(
        output_file_path=absolute_path.replace(".py", ".png")
    )
except Exception as e:
    # graphviz ë¯¸ì„¤ì¹˜ ë“±ìœ¼ë¡œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡°ìš©íˆ íŒ¨ìŠ¤
    pass


# =========================
# ì´ˆê¸° State
# =========================
def initial_state() -> State:
    return State(
        messages=[
            SystemMessage(
                content=f"""
            ë„ˆí¬ AIë“¤ì€ ì‚¬ìš©ìì˜ ìš”êµ¬ì— ë§ëŠ” ì±…ì„ ì“°ëŠ” ì‘ê°€íŒ€ì´ë‹¤.
            ì‚¬ìš©ìê°€ ì‚¬ìš©í•˜ëŠ” ì–¸ì–´ë¡œ ëŒ€í™”í•˜ë¼.

            í˜„ì¬ì‹œê°ì€ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}ì´ë‹¤.
            """
            )
        ]
    )


# =========================
# Gradio UI
# =========================
"""
êµ¬ì„±:
- ì¢Œì¸¡: Chatbot (ëŒ€í™” ê¸°ë¡)
- í•˜ë‹¨: ì‚¬ìš©ì ì…ë ¥ì°½ + ì „ì†¡ ë²„íŠ¼
- ìš°ì¸¡: ìƒíƒœ ì •ë³´(ë©”ì‹œì§€ ì¹´ìš´íŠ¸, ì €ì¥ ê²½ë¡œ ì•ˆë‚´) ë° ì»¨íŠ¸ë¡¤(ì´ˆê¸°í™” ë²„íŠ¼)

ë™ì‘:
1) ì‚¬ìš©ìê°€ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê³  ì „ì†¡í•˜ë©´,
2) í•´ë‹¹ ì…ë ¥ì„ State.messages ì— HumanMessage ë¡œ ì¶”ê°€,
3) LangGraph íŒŒì´í”„ë¼ì¸(graph.invoke) ì‹¤í–‰,
4) Communicator ê°€ ìƒì„±í•œ ìµœì‹  AI ì‘ë‹µì„ Chatbot ì— ë°˜ì˜.
"""

with gr.Blocks(title="OpenCode â€¢ DeepSeek-R1 (Ollama) â€¢ LangGraph + Gradio", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ğŸ“š OpenCode: LangGraph + DeepSeek-R1 (Ollama) + Gradio  
        - **Closed LLM â†’ ì˜¤í”ˆì†ŒìŠ¤ LLM ë³€í™˜ë³¸**  
        - ë¡œì»¬ **Ollama** + **DeepSeek-R1** ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©°, **API Key ë¶ˆí•„ìš”**  
        - ì½˜í…ì¸  ì „ëµê°€ âœ ì»¤ë®¤ë‹ˆì¼€ì´í„° ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ëª©ì°¨/ëŒ€í™” ì§„í–‰
        """
    )

    with gr.Row():
        with gr.Column(scale=3):
            chat = gr.Chatbot(height=520, label="ëŒ€í™”ì°½")
            user_in = gr.Textbox(
                placeholder="ì—¬ê¸°ì— ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: ì±•í„° 2ëŠ” ì˜ˆì œë¡œ ì±„ì›Œì£¼ì„¸ìš”)",
                label="User ì…ë ¥",
            )
            send_btn = gr.Button("ì „ì†¡", variant="primary")

        with gr.Column(scale=2):
            info = gr.Markdown(
                f"**ì‘ì—… í´ë”**: `{current_path}`\n\n"
                " - outlineì€ `_runs/outline.md`ì— ì €ì¥ë©ë‹ˆë‹¤.\n"
                " - stateëŠ” `_runs/state.txt`ì— ì €ì¥ë©ë‹ˆë‹¤.\n"
            )
            msg_count = gr.Number(value=1, precision=0, label="ë©”ì‹œì§€ ê°œìˆ˜", interactive=False)
            reset_btn = gr.Button("ì„¸ì…˜ ì´ˆê¸°í™”", variant="secondary")

    # ì„¸ì…˜ ë³´ê´€ìš© (State ê°ì²´)
    st = gr.State(initial_state())

    def on_reset():
        s = initial_state()
        return s, [], 1

    reset_btn.click(on_reset, inputs=None, outputs=[st, chat, msg_count])

    def on_send(user_text: str, s: State, history: list[list[str, str]]):
        """
        Gradio í•¸ë“¤ëŸ¬:
        - user_text ë¥¼ HumanMessage ë¡œ ì¶”ê°€
        - LangGraph ì‹¤í–‰
        - ë§ˆì§€ë§‰ AI ì‘ë‹µì„ history ì— ë°˜ì˜
        """
        user_text = (user_text or "").strip()
        if not user_text:
            return gr.update(), s, history, len(s["messages"])

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        s["messages"].append(HumanMessage(user_text))
        history = history + [[user_text, None]]

        # LangGraph íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        new_state = graph.invoke(s)

        # ìµœì‹  AI ì‘ë‹µ ì¶”ì¶œ
        ai_text = ""
        for m in reversed(new_state["messages"]):
            if isinstance(m, AIMessage):
                ai_text = m.content
                break
            elif hasattr(m, "content") and m.__class__.__name__ == "AIMessageChunk":
                # í˜¹ì‹œ chunk ë¡œ ë‚¨ì•„ ìˆëŠ” ê²½ìš° ëŒ€ë¹„
                ai_text = getattr(m, "content", "")
                break

        # Chatbot ìµœì‹  í„´ ì±„ìš°ê¸°
        if history and history[-1][1] is None:
            history[-1][1] = ai_text or "(ì‘ë‹µ ì—†ìŒ)"

        # ìƒíƒœ ì €ì¥
        try:
            save_state(current_path, new_state)
        except Exception:
            pass

        return "", new_state, history, len(new_state["messages"])

    send_btn.click(
        on_send,
        inputs=[user_in, st, chat],
        outputs=[user_in, st, chat, msg_count],
    )

    # Enter ë¡œë„ ì „ì†¡
    user_in.submit(
        on_send,
        inputs=[user_in, st, chat],
        outputs=[user_in, st, chat, msg_count],
    )


# =========================
# CLI ëª¨ë“œ (ì˜µì…˜)
# =========================
if __name__ == "__main__":
    # Gradio ì„œë²„ ì‹¤í–‰
    # - share=True: ì™¸ë¶€ì—ì„œ ì ‘ì† í•„ìš” ì‹œ ì‚¬ìš© (ê¸°ë³¸ì€ ë¡œì»¬ë§Œ)
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)
