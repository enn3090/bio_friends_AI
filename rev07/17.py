"""
[OpenCode ë³€í™˜ë³¸]

ëª©í‘œ: Streamlit + OpenAI(Closed LLM) ê¸°ë°˜ RAG ì±—ë´‡ ì½”ë“œë¥¼
â†’ **ë¡œì»¬ ì˜¤í”ˆì†ŒìŠ¤ LLM (Ollamaì˜ DeepSeek-R1)** + **Gradio UI**ë¡œ ë³€í™˜

í•µì‹¬ ë³€ê²½ì 
1) OpenAI/í‚¤ ì‚­ì œ, **Ollama ë¡œì»¬** ì‚¬ìš© (ê¸°ë³¸: http://localhost:11434)
2) ëª¨ë¸: deepseek-r1 (ì˜ˆ: "deepseek-r1:latest" ë˜ëŠ” "deepseek-r1:7b/14b")
3) Streamlit â†’ **Gradio ChatInterface** (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
4) ê¸°ì¡´ retriever ëª¨ë“ˆ(query_augmentation_chain, retriever, document_chain) **ê·¸ëŒ€ë¡œ ì‚¬ìš©**
5) DeepSeek-R1ì´ í† ì¶œí•˜ëŠ” ë‚´ë¶€ ì‚¬ê³ (<think>...</think>)ëŠ” ì‚¬ìš©ì ì¶œë ¥ì—ì„œ ìë™ ì œê±°

ì‚¬ì „ ì¤€ë¹„
- Ollama ì„¤ì¹˜: https://ollama.com
- ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: `ollama pull deepseek-r1:latest`
- íŒŒì´ì¬ íŒ¨í‚¤ì§€: `pip install gradio langchain langchain-community`
- (ì¤‘ìš”) `retriever.py` ëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ëª¨ë“ˆ ê²½ë¡œì— ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤.
"""

from __future__ import annotations
from typing import List, Tuple, Generator
import re
import textwrap

import gradio as gr

# LangChain ë©”ì‹œì§€ íƒ€ì… (ì› ì½”ë“œì™€ ë™ì¼)
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

# LangChainìš© Ollama ì±— ëª¨ë¸
from langchain_community.chat_models import ChatOllama

import retriever  # ê¸°ì¡´ ëª¨ë“ˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©


# ====== ì„¤ì • ======
MODEL_NAME = "deepseek-r1:latest"  # í•„ìš” ì‹œ "deepseek-r1:7b" ë“±ìœ¼ë¡œ ë³€ê²½
TEMPERATURE = 0.7
SYSTEM_PROMPT = "ë„ˆëŠ” ë¬¸ì„œì— ê¸°ë°˜í•´ ë‹µë³€í•˜ëŠ” ë„ì‹œ ì •ì±… ì „ë¬¸ê°€ì•¼"

# DeepSeek-R1ì˜ ë‚´ë¶€ ì‚¬ê³ (<think>...</think>) ì œê±°ìš©
THINK_TAG_RE = re.compile(r"<think>.*?</think>", re.DOTALL)


def strip_think(text: str) -> str:
    """ì™„ì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ <think> ë¸”ë¡ì„ ì œê±°."""
    return THINK_TAG_RE.sub("", text).strip()


def streaming_hide_think(chunks: Generator[str, None, None]) -> Generator[str, None, None]:
    """
    ìŠ¤íŠ¸ë¦¬ë° ì¤‘ê°„ì—ë„ <think> ì•ˆì˜ ë‚´ìš©ì´ í™”ë©´ì— ë³´ì´ì§€ ì•Šë„ë¡ í•„í„°ë§.
    - ìƒíƒœ ë¨¸ì‹  ë°©ì‹ìœ¼ë¡œ '<think>' ~ '</think>' ì‚¬ì´ë¥¼ ìˆ¨ê¹€.
    """
    in_think = False
    buffer = ""

    for raw in chunks:
        # LangChain ì¼ë¶€ ë“œë¼ì´ë²„ê°€ dict/CallbackEventë¥¼ ë‚´ë³´ë‚´ëŠ” ê²½ìš°ë„ ìˆì–´ ë°©ì–´ì½”ë”©
        s = str(raw)

        buffer += s
        out = []

        i = 0
        while i < len(buffer):
            if not in_think:
                # '<think>' ì‹œì‘ í† í° íƒìƒ‰
                start = buffer.find("<think>", i)
                if start == -1:
                    # ë‚¨ì€ ì „ë¶€ ì¶œë ¥ í›„ë³´
                    out.append(buffer[i:])
                    buffer = ""  # ë²„í¼ ë¹„ì›€
                    break
                else:
                    # ì‹œì‘ ì „ê¹Œì§€ ì¶œë ¥ â†’ ì´í›„ë¶€í„°ëŠ” think ëª¨ë“œ
                    out.append(buffer[i:start])
                    i = start + len("<think>")
                    in_think = True
            else:
                # think ë¸”ë¡ ì¢…ë£Œ í† í° íƒìƒ‰
                end = buffer.find("</think>", i)
                if end == -1:
                    # ì¢…ë£Œ ì—†ìœ¼ë©´ ë‚˜ë¨¸ì§€ëŠ” ë²„í¼ì— ë‚¨ê²¨ë‘ê³  ë” ë°›ê¸°
                    buffer = buffer[i:]
                    i = len(buffer)
                    break
                else:
                    # ì¢…ë£Œ ì§€ì ê¹Œì§€ëŠ” ìˆ¨ê¸°ê³ , ì¢…ë£Œ ë’¤ë¡œ ì§„í–‰
                    i = end + len("</think>")
                    in_think = False
                    # think ë¸”ë¡ ì´í›„ í…ìŠ¤íŠ¸ê°€ ë’¤ì— ë” ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ê³„ì† ë£¨í”„

        visible = "".join(out)
        if visible:
            yield visible


# ====== LLM ì¸ìŠ¤í„´ìŠ¤ (ë¡œì»¬ Ollama) ======
llm = ChatOllama(
    model=MODEL_NAME,
    temperature=TEMPERATURE,
    # base_url="http://localhost:11434",  # ê¸°ë³¸ê°’ì´ë©´ ì£¼ì„ ê°€ëŠ¥
)


# ====== Gradio <-> LangChain íˆìŠ¤í† ë¦¬ ë³€í™˜ ======
def history_to_messages(history: List[Tuple[str, str]]) -> List:
    """
    Gradio ChatInterfaceì˜ history([[user, assistant], ...])ë¥¼ LangChain ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜.
    í•­ìƒ SYSTEM_PROMPTë¥¼ ë§¨ ì•ì— ì¶”ê°€.
    """
    msgs: List = [SystemMessage(SYSTEM_PROMPT)]
    for user_text, ai_text in history:
        if user_text:
            msgs.append(HumanMessage(user_text))
        if ai_text:
            msgs.append(AIMessage(ai_text))
    return msgs


# ====== ë¬¸ì„œ íŒ¨ë„ìš© ë§ˆí¬ë‹¤ìš´ ìƒì„± ======
def format_docs_markdown(docs) -> str:
    """
    retriever ê²°ê³¼(docs)ë¥¼ ì ‘ì´ì‹ ì„¹ì…˜ìœ¼ë¡œ í‘œì‹œí•  ë§ˆí¬ë‹¤ìš´ ìƒì„±.
    - ChatInterface ì˜ additional_outputs ë¡œ ì˜¤ë¥¸ìª½ íŒ¨ë„ ê°±ì‹ ì— ì‚¬ìš©.
    """
    parts = ["### ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ (RAG ì»¨í…ìŠ¤íŠ¸)\n"]
    for idx, doc in enumerate(docs, 1):
        src = doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
        page = doc.metadata.get("page", "")
        header = f"**{idx}. ì†ŒìŠ¤:** `{src}`  â€”  **page:** {page}"
        body = textwrap.indent(doc.page_content.strip(), prefix="> ")
        parts.append(f"{header}\n\n{body}\n")
        parts.append("---")
    return "\n".join(parts) if len(docs) > 0 else "_ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤._"


# ====== ë©”ì¸ ì‘ë‹µ ë¡œì§ ======
def respond(user_input: str, history: List[Tuple[str, str]]):
    """
    ChatInterface ì½œë°±.
    1) íˆìŠ¤í† ë¦¬ + ì‚¬ìš©ì ì§ˆë¬¸ìœ¼ë¡œ ì§ˆì˜ í™•ì¥(augmentation)
    2) ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ë° ìš°ì¸¡ íŒ¨ë„ ê°±ì‹ 
    3) document_chain.stream ìœ¼ë¡œ ìƒì„± ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°(+ <think> ìˆ¨ê¹€)
    ë°˜í™˜ í˜•ì‹:
      - ìŠ¤íŠ¸ë¦¬ë° ì‹œ: yield (partial_text, docs_markdown)
      - ì¢…ë£Œ ì‹œ: return ìµœì¢… ì „ì²´ í…ìŠ¤íŠ¸
    """
    # 1) ë©”ì‹œì§€ êµ¬ì„±
    messages = history_to_messages(history) + [HumanMessage(user_input)]

    # 2) ì§ˆì˜ í™•ì¥
    try:
        augmented_query = retriever.query_augmentation_chain.invoke({
            "messages": messages,
            "query": user_input,
        })
    except Exception as e:
        augmented_query = ""
        # ì‹¤íŒ¨í•´ë„ RAGëŠ” ì§„í–‰ (ë¬¸ì œ ì›ì¸ ì¶œë ¥ì€ ìµœì†Œí™”)
    
    # 3) ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    try:
        docs = retriever.retriever.invoke(f"{user_input}\n{augmented_query}")
    except Exception:
        docs = []

    docs_md = format_docs_markdown(docs)

    # 4) ìŠ¤íŠ¸ë¦¬ë° ìƒì„± (retriever.document_chain.stream)
    try:
        raw_stream = retriever.document_chain.stream({
            "messages": messages,
            "context": docs,
        })
    except Exception as e:
        # document_chain ë™ì‘ ì‹¤íŒ¨ ì‹œ, ìµœì†Œí•œì˜ ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜
        err_msg = f"ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆì–´ìš”. ì‚¬ìœ : {e}"
        yield err_msg, docs_md
        return

    # ë‚´ë¶€ ì‚¬ê³  ìˆ¨ê¹€ ìŠ¤íŠ¸ë¦¬ë°
    filtered_stream = streaming_hide_think(raw_stream)

    # GradioëŠ” (ë¶€ë¶„ë¬¸ìì—´, ì¶”ê°€ì¶œë ¥) í˜•íƒœì˜ íŠœí”Œì„ yield í•´ë„ ëœë‹¤.
    for piece in filtered_stream:
        yield piece, docs_md  # docs íŒ¨ë„ì€ ì „ì²´ ê³¼ì • ë™ì•ˆ ë™ì¼í•˜ê²Œ ìœ ì§€

    # ìŠ¤íŠ¸ë¦¬ë°ì´ ëë‚œ ë’¤ ì¶”ê°€ ë°˜í™˜ì€ ìƒëµ ê°€ëŠ¥ (ìœ„ì˜ yield ë“¤ë¡œ ì´ë¯¸ ì™„ì„±ë¨)


# ====== Gradio UI êµ¬ì„± ======
with gr.Blocks(css="footer {visibility: hidden}") as demo:
    gr.Markdown("## ğŸ™ï¸ ë¬¸ì„œê¸°ë°˜ ë„ì‹œì •ì±… ì±—ë´‡ (DeepSeek-R1 on Ollama + Gradio)")

    with gr.Row():
        chat = gr.ChatInterface(
            fn=respond,
            additional_outputs=[gr.Markdown(label="RAG ì»¨í…ìŠ¤íŠ¸")],
            chatbot=gr.Chatbot(
                label="ë„ì‹œì •ì±… ì „ë¬¸ê°€",
                height=520,
            ),
            title="",
            description=(
                "ë¡œì»¬ **Ollama**ì˜ **DeepSeek-R1** ëª¨ë¸ê³¼ ê¸°ì¡´ `retriever` ì²´ì¸ì„ ì´ìš©í•´ "
                "ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n"
                "â€» ë‚´ë¶€ ì‚¬ê³ (<think>...</think>)ëŠ” ìë™ìœ¼ë¡œ ìˆ¨ê²¨ì§‘ë‹ˆë‹¤."
            ),
            theme="soft",
            retry_btn="ë‹¤ì‹œ ìƒì„±",
            undo_btn="ì´ì „ ë©”ì‹œì§€ ì‚­ì œ",
            clear_btn="ëŒ€í™” ì´ˆê¸°í™”",
            examples=[
                "ë„ì‹œ ì—´ì„¬ í˜„ìƒì„ ì™„í™”í•˜ê¸° ìœ„í•œ ë‹¨ê¸° ì •ì±…ì€?",
                "ë³´í–‰ì¹œí™”êµ¬ì—­ ì§€ì • ì‹œ ê³ ë ¤í•´ì•¼ í•  ì§€í‘œë¥¼ ì•Œë ¤ì¤˜.",
                "ìš°ë¦¬ êµ¬ì˜ ì£¼ì°¨ ìˆ˜ìš” ê´€ë¦¬ ë°©ì•ˆ ì‚¬ë¡€ê°€ ìˆì„ê¹Œ?",
            ],
        )

    with gr.Accordion("ì‹œìŠ¤í…œ/ëª¨ë¸ ì •ë³´", open=False):
        gr.Markdown(
            f"""
- ì‚¬ìš© ëª¨ë¸: **{MODEL_NAME}**  
- ë¡œì»¬ ì„œë²„: **http://localhost:11434** (Ollama ê¸°ë³¸)  
- ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: `{SYSTEM_PROMPT}`  
- Temperature: `{TEMPERATURE}`  
- ì‚¬ìš© ëª¨ë“ˆ: `retriever.query_augmentation_chain`, `retriever.retriever`, `retriever.document_chain`
"""
        )

if __name__ == "__main__":
    # ë‚´ë¶€ë§ë§Œ ì‚¬ìš©í•  ê²½ìš° share=False ê¶Œì¥
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)

