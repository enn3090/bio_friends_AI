# -*- coding: utf-8 -*-
"""
[OpenCode ë³€í™˜ë³¸: Closed LLM(OpenAI) â†’ ì˜¤í”ˆì†ŒìŠ¤ LLM(Ollama â€¢ DeepSeekâ€‘R1) + Gradio GUI]

ë³¸ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì›ë³¸ ì½”ë“œì—ì„œ `ChatOpenAI` ì˜ì¡´ì„±ì„ ì œê±°í•˜ê³ ,
ë¡œì»¬ì—ì„œ êµ¬ë™ë˜ëŠ” **Ollama + DeepSeekâ€‘R1** ëª¨ë¸ì„ ì‚¬ìš©í•˜ë„ë¡ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.
ë˜í•œ í„°ë¯¸ë„ ì…ë ¥ ë£¨í”„ ëŒ€ì‹  **Gradio** ê¸°ë°˜ì˜ ê°„ë‹¨í•œ ê·¸ë˜í”½ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì „ì œ:
- ë¡œì»¬ PCì— Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆê³ , deepseek-r1 ê³„ì—´ ëª¨ë¸ì„ pull ì™„ë£Œ
  ì˜ˆ) `ollama pull deepseek-r1:7b` ë˜ëŠ” `ollama pull deepseek-r1:32b`
- `utils.py`ì— `save_state`, `get_outline`, `save_outline` í•¨ìˆ˜ê°€ ì¡´ì¬(ì—†ì–´ë„ ì•ˆì „ í´ë°± ì œê³µ)
- API Key ë¶ˆí•„ìš” (sk-... ì¼ì²´ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)

ì„¤ì¹˜:
    pip install langchain langgraph gradio
    pip install langchain-ollama  # (ê¶Œì¥)
    # ë˜ëŠ” í™˜ê²½ì— ë”°ë¼
    pip install langchain-community

ì‹¤í–‰:
    python opencode_supervisor_langgraph_ollama_gradio.py
"""

from __future__ import annotations

import os
from datetime import datetime
from typing import List

# LangGraph / LangChain
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict

# ë©”ì‹œì§€ íƒ€ì…/í”„ë¡¬í”„íŠ¸/íŒŒì„œ
from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers.string import StrOutputParser

# Ollama(ë¡œì»¬ LLM) â€” íŒ¨í‚¤ì§€ ë²„ì „ì— ë”°ë¼ import ê²½ë¡œê°€ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ try/except ì²˜ë¦¬
try:
    from langchain_ollama import ChatOllama  # ìµœì‹  ê¶Œì¥
except Exception:
    try:
        from langchain_community.chat_models import ChatOllama  # ëŒ€ì²´ ê²½ë¡œ
    except Exception as e:
        raise ImportError(
            "ChatOllama ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `pip install langchain-ollama`\n"
            "ë˜ëŠ” `pip install langchain-community`ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”."
        )

# GUI
import gradio as gr

# utils.py ì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from utils import save_state, get_outline, save_outline


# =========================
# ê²½ë¡œ/íŒŒì¼ ì •ë³´
# =========================
try:
    FILENAME = os.path.basename(__file__)
    ABS_PATH = os.path.abspath(__file__)
    CUR_PATH = os.path.dirname(ABS_PATH)
except NameError:
    # ë…¸íŠ¸ë¶/REPL ë“±ì—ì„œ __file__ ì´ ì—†ì„ ê²½ìš°
    FILENAME = "opencode_supervisor_langgraph_ollama_gradio.py"
    ABS_PATH = os.path.abspath("./" + FILENAME)
    CUR_PATH = os.path.dirname(ABS_PATH)


# =========================
# ë¡œì»¬ LLM ì´ˆê¸°í™” (DeepSeekâ€‘R1 on Ollama)
# =========================
# í•„ìš” ì‹œ model íƒœê·¸ë¥¼ í™˜ê²½ì— ë§ê²Œ ë³€ê²½í•˜ì„¸ìš”. (ì˜ˆ: "deepseek-r1:32b")
llm = ChatOllama(
    model="deepseek-r1:7b",
    temperature=0.7,
    # ì•„ë˜ íŒŒë¼ë¯¸í„°ëŠ” Ollama ë° LangChain ë²„ì „ì— ë”°ë¼ ì§€ì› ì—¬ë¶€ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # num_ctx=4096,
    # top_p=0.9,
)


# =========================
# ìƒíƒœ ì •ì˜
# =========================
class State(TypedDict):
    messages: List[AnyMessage | str]
    task: str


# =========================
# Supervisor ì—ì´ì „íŠ¸ â€” ì–´ë–¤ ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí• ì§€ ê²°ì •
# =========================

def supervisor(state: State):
    print("\n\n============ SUPERVISOR ============")

    supervisor_system_prompt = PromptTemplate.from_template(
        """
        ë„ˆëŠ” AI íŒ€ì˜ supervisorë¡œì„œ AI íŒ€ì˜ ì‘ì—…ì„ ê´€ë¦¬í•˜ê³  ì§€ë„í•œë‹¤.
        ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì±…ì„ ì¨ì•¼ í•œë‹¤ëŠ” ìµœì¢… ëª©í‘œë¥¼ ì—¼ë‘ì— ë‘ê³ , 
        ì‚¬ìš©ìì˜ ìš”êµ¬ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ í˜„ì¬ í•´ì•¼í•  ì¼ì´ ë¬´ì—‡ì¸ì§€ ê²°ì •í•œë‹¤.

        supervisorê°€ í™œìš©í•  ìˆ˜ ìˆëŠ” agentëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.     
        - content_strategist: ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ì´ ëª…í™•í•´ì¡Œì„ ë•Œ ì‚¬ìš©í•œë‹¤. AI íŒ€ì˜ ì½˜í…ì¸  ì „ëµì„ ê²°ì •í•˜ê³ , ì „ì²´ ì±…ì˜ ëª©ì°¨(outline)ë¥¼ ì‘ì„±í•œë‹¤. 
        - communicator: AI íŒ€ì—ì„œ í•´ì•¼ í•  ì¼ì„ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•  ìˆ˜ ì—†ì„ ë•Œ ì‚¬ìš©í•œë‹¤. ì‚¬ìš©ìì—ê²Œ ì§„í–‰ìƒí™©ì„ ë³´ê³ í•˜ê³ , ë‹¤ìŒ ì§€ì‹œë¥¼ ë¬¼ì–´ë³¸ë‹¤. 

        ì•„ë˜ ë‚´ìš©ì„ ê³ ë ¤í•˜ì—¬, í˜„ì¬ í•´ì•¼í•  ì¼ì´ ë¬´ì—‡ì¸ì§€, ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” agentë¥¼ ë‹¨ë‹µìœ¼ë¡œ ë§í•˜ë¼.

        ------------------------------------------
        previous_outline: {outline}
        ------------------------------------------
        messages:
        {messages}
        """
    )

    chain = supervisor_system_prompt | llm | StrOutputParser()

    messages = state.get("messages", [])
    inputs = {"messages": messages, "outline": get_outline(CUR_PATH)}

    # ë‹¨ë°œ í˜¸ì¶œ(ìŠ¤íŠ¸ë¦¬ë° ë¶ˆí•„ìš”)ë¡œ task ê²°ì •
    task = chain.invoke(inputs).strip()

    # Supervisor ë¡œê·¸ ë©”ì‹œì§€
    sup_msg = AIMessage(content=f"[Supervisor] {task}")
    messages.append(sup_msg)
    print(sup_msg.content)

    return {"messages": messages, "task": task}


# =========================
# Router â€” supervisor ì¶œë ¥ì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë¡œ ì´ë™
# =========================

def supervisor_router(state: State):
    task = (state.get("task") or "").strip().lower()
    # supervisorê°€ ì •í™•íˆ ë…¸ë“œ ì´ë¦„ì„ ë§í•œë‹¤ê³  ê°€ì •í•˜ë˜, ì•½ê°„ì˜ ê´€ìš© ì²˜ë¦¬
    if "content" in task:
        return "content_strategist"
    if "communicator" in task or task == "ask" or "ì‚¬ìš©ì" in task:
        return "communicator"
    # ê¸°ë³¸ê°’: communicatorë¡œ ìœ ë„í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì§ˆì˜
    return "communicator"


# =========================
# ì½˜í…ì¸  ì „ëµê°€ â€” ëª©ì°¨ ì‘ì„±/ê°±ì‹  í›„ ì €ì¥
# =========================

def content_strategist(state: State):
    print("\n\n============ CONTENT STRATEGIST ============")

    content_strategist_system_prompt = PromptTemplate.from_template(
        """
        ë„ˆëŠ” ì±…ì„ ì“°ëŠ” AIíŒ€ì˜ ì½˜í…ì¸  ì „ëµê°€(Content Strategist)ë¡œì„œ,
        ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ê³ , AIíŒ€ì´ ì“¸ ì±…ì˜ ì„¸ë¶€ ëª©ì°¨ë¥¼ ê²°ì •í•œë‹¤.

        ì§€ë‚œ ëª©ì°¨ê°€ ìˆë‹¤ë©´ ê·¸ ë²„ì „ì„ ì‚¬ìš©ìì˜ ìš”êµ¬ì— ë§ê²Œ ìˆ˜ì •í•˜ê³ , ì—†ë‹¤ë©´ ìƒˆë¡œìš´ ëª©ì°¨ë¥¼ ì œì•ˆí•œë‹¤.

        --------------------------------
        - ì§€ë‚œ ëª©ì°¨: {outline}
        --------------------------------
        - ì´ì „ ëŒ€í™” ë‚´ìš©: {messages}
        """
    )

    chain = content_strategist_system_prompt | llm | StrOutputParser()

    messages = state["messages"]
    outline = get_outline(CUR_PATH)
    inputs = {"messages": messages, "outline": outline}

    # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ëª©ì°¨ ìƒì„± (ì½˜ì†” ì¶œë ¥)
    gathered = ""
    for chunk in chain.stream(inputs):
        gathered += chunk
        print(chunk, end="")
    print()

    # ëª©ì°¨ ì €ì¥
    save_outline(CUR_PATH, gathered)

    # ì§„í–‰ ìƒí™© ë©”ì‹œì§€
    done_msg = "[Content Strategist] ëª©ì°¨ ì‘ì„± ì™„ë£Œ"
    print(done_msg)
    messages.append(AIMessage(content=done_msg))

    return {"messages": messages}


# =========================
# ì»¤ë®¤ë‹ˆì¼€ì´í„° â€” ì‚¬ìš©ìì™€ ëŒ€í™”(ëª©ì°¨ ì „ë¬¸ì€ ì¬ì¶œë ¥í•˜ì§€ ì•ŠìŒ)
# =========================

def communicator(state: State):
    print("\n\n============ COMMUNICATOR ============")

    communicator_system_prompt = PromptTemplate.from_template(
        """
        ë„ˆëŠ” ì±…ì„ ì“°ëŠ” AIíŒ€ì˜ ì»¤ë®¤ë‹ˆì¼€ì´í„°ë¡œì„œ, 
        AIíŒ€ì˜ ì§„í–‰ìƒí™©ì„ ì‚¬ìš©ìì—ê²Œ ë³´ê³ í•˜ê³ , ì‚¬ìš©ìì˜ ì˜ê²¬ì„ íŒŒì•…í•˜ê¸° ìœ„í•œ ëŒ€í™”ë¥¼ ë‚˜ëˆˆë‹¤. 

        ì‚¬ìš©ìë„ outline(ëª©ì°¨)ì„ ì´ë¯¸ ë³´ê³  ìˆìœ¼ë¯€ë¡œ, ë‹¤ì‹œ ì¶œë ¥í•  í•„ìš”ëŠ” ì—†ë‹¤.

        messages: {messages}
        """
    )

    chain = communicator_system_prompt | llm

    messages = state["messages"]
    inputs = {"messages": messages}

    print("\nAI\t: ", end="")
    gathered = None
    for chunk in chain.stream(inputs):
        text = getattr(chunk, "content", "") if chunk else ""
        print(text, end="")
        if gathered is None:
            gathered = AIMessage(content=text)
        else:
            gathered.content += text

    if gathered is None:
        gathered = AIMessage(content="(ì‘ë‹µ ì—†ìŒ)")

    messages.append(gathered)
    return {"messages": messages}


# =========================
# ê·¸ë˜í”„ êµ¬ì„±
# =========================

graph_builder = StateGraph(State)

graph_builder.add_node("supervisor", supervisor)
graph_builder.add_node("communicator", communicator)
graph_builder.add_node("content_strategist", content_strategist)

# íë¦„: START â†’ supervisor â†’ (ì¡°ê±´ë¶€) content_strategist ë˜ëŠ” communicator â†’ communicator â†’ END
graph_builder.add_edge(START, "supervisor")

graph_builder.add_conditional_edges(
    "supervisor",
    supervisor_router,
    {
        "content_strategist": "content_strategist",
        "communicator": "communicator",
    },
)

# ëª©ì°¨ ì‘ì„± í›„ì—ëŠ” ì‚¬ìš©ì ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ìœ¼ë¡œ ì—°ê²°
graph_builder.add_edge("content_strategist", "communicator")

# ìµœì¢…ì ìœ¼ë¡œ communicator ì—ì„œ ì¢…ë£Œ
graph_builder.add_edge("communicator", END)

graph = graph_builder.compile()

# ì‹œê°í™”(ì˜µì…˜)
try:
    graph.get_graph().draw_mermaid_png(output_file_path=ABS_PATH.replace(".py", ".png"))
except Exception:
    pass


# =========================
# ì´ˆê¸° State
# =========================

def initial_state() -> State:
    return State(
        messages=[
            SystemMessage(
                content=f"""
            ë„ˆí¬ AIë“¤ì€ ì‚¬ìš©ìì˜ ìš”êµ¬ì— ë§ëŠ” ì±…ì„ ì“°ëŠ” ì‘ê°€íŒ€ì´ë‹¤.
            ì‚¬ìš©ìê°€ ì‚¬ìš©í•˜ëŠ” ì–¸ì–´ë¡œ ëŒ€í™”í•˜ë¼.

            í˜„ì¬ì‹œê°ì€ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}ì´ë‹¤.
            """
            )
        ],
        task="",
    )


# =========================
# Gradio UI
# =========================
"""
êµ¬ì„±:
- ì¢Œì¸¡: Chatbot (ëŒ€í™” ê¸°ë¡)
- í•˜ë‹¨: ì‚¬ìš©ì ì…ë ¥ì°½ + ì „ì†¡ ë²„íŠ¼
- ìš°ì¸¡: í˜„ì¬ Supervisor íŒë‹¨(task), ë©”ì‹œì§€ ìˆ˜, ì•ˆë‚´/ì´ˆê¸°í™”

ë™ì‘:
1) ì‚¬ìš©ìê°€ ì…ë ¥ â†’ HumanMessage ë¡œ ì¶”ê°€
2) LangGraph(graph.invoke) ì‹¤í–‰ â†’ supervisor ê°€ ë¼ìš°íŒ… ê²°ì •
3) content_strategist ê°€ í•„ìš” ì‹œ ëª©ì°¨ ì €ì¥ í›„ communicator ê°€ ì‚¬ìš©ì ì‘ë‹µ ìƒì„±
4) ìµœì‹  AI ì‘ë‹µì„ Chatbot ì— ë°˜ì˜, state ì €ì¥
"""

with gr.Blocks(title="OpenCode â€¢ DeepSeekâ€‘R1 (Ollama) â€¢ LangGraph + Gradio", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ğŸ“š OpenCode: LangGraph + DeepSeekâ€‘R1 (Ollama) + Gradio  
        - **Closed LLM â†’ ì˜¤í”ˆì†ŒìŠ¤ LLM ë³€í™˜ë³¸** - ë¡œì»¬ **Ollama** + **DeepSeekâ€‘R1** ëª¨ë¸ ì‚¬ìš© (API Key ë¶ˆí•„ìš”)  
        - Supervisor â†’ (Content Strategist | Communicator) â†’ Communicator íŒŒì´í”„ë¼ì¸
        """
    )

    with gr.Row():
        with gr.Column(scale=3):
            chat = gr.Chatbot(height=520, label="ëŒ€í™”ì°½")
            user_in = gr.Textbox(placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...", label="User ì…ë ¥")
            send_btn = gr.Button("ì „ì†¡", variant="primary")
        with gr.Column(scale=2):
            task_box = gr.Textbox(label="Supervisor íŒë‹¨(ë¼ìš°íŒ…)", interactive=False)
            gr.Markdown(
                f"**ì‘ì—… í´ë”**: `{CUR_PATH}`\n\n"
                " - outlineì€ `_runs/outline.md`ì— ì €ì¥ë©ë‹ˆë‹¤.\n"
                " - stateëŠ” `_runs/state.txt`ì— ì €ì¥ë©ë‹ˆë‹¤.\n"
            )
            msg_count = gr.Number(value=1, precision=0, label="ë©”ì‹œì§€ ê°œìˆ˜", interactive=False)
            reset_btn = gr.Button("ì„¸ì…˜ ì´ˆê¸°í™”", variant="secondary")

    # ì„¸ì…˜ ìƒíƒœ
    st = gr.State(initial_state())

    def on_reset():
        s = initial_state()
        return s, [], "", 1

    reset_btn.click(on_reset, inputs=None, outputs=[st, chat, task_box, msg_count])

    def on_send(user_text: str, s: State, history: list[list[str, str]]):
        """Gradio ì „ì†¡ í•¸ë“¤ëŸ¬"""
        user_text = (user_text or "").strip()
        if not user_text:
            return gr.update(), s, history, (s.get("task") or ""), len(s["messages"])

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        s["messages"].append(HumanMessage(user_text))
        history = history + [[user_text, None]]

        # ê·¸ë˜í”„ ì‹¤í–‰ (STARTâ†’END ì „ì²´ í”Œë¡œìš°)
        new_state = graph.invoke(s)

        # ìµœì‹  AI ì‘ë‹µ ì°¾ê¸° (communicatorì˜ ì‘ë‹µ)
        ai_text = ""
        for m in reversed(new_state["messages"]):
            if isinstance(m, AIMessage):
                ai_text = m.content
                break

        # Chatbot ìµœì‹  í„´ ì±„ìš°ê¸°
        if history and history[-1][1] is None:
            history[-1][1] = ai_text or "(ì‘ë‹µ ì—†ìŒ)"

        # ìƒíƒœ ì €ì¥
        try:
            save_state(CUR_PATH, new_state)
        except Exception:
            pass

        # Supervisorì˜ ìµœì¢… íŒë‹¨ê°’ í‘œì‹œ
        task_display = (new_state.get("task") or "").strip()

        return "", new_state, history, task_display, len(new_state["messages"])

    send_btn.click(on_send, inputs=[user_in, st, chat], outputs=[user_in, st, chat, task_box, msg_count])
    user_in.submit(on_send, inputs=[user_in, st, chat], outputs=[user_in, st, chat, task_box, msg_count])


# =========================
# ë©”ì¸ ì§„ì… â€” Gradio ì„œë²„ ì‹¤í–‰
# =========================
if __name__ == "__main__":
    # ì™¸ë¶€ ì ‘ê·¼ì´ í•„ìš”í•˜ë©´ share=True ë˜ëŠ” server_name ìˆ˜ì •
    try:
        # ê·¸ë˜í”„ êµ¬ì¡° ì´ë¯¸ì§€ ì €ì¥ ì‹œë„ (ì˜µì…˜)
        graph.get_graph().draw_mermaid_png(output_file_path=ABS_PATH.replace(".py", ".png"))
    except Exception:
        pass

    # Gradio ëŸ°ì¹˜
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)