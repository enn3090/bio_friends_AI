"""
[OpenCode ë³€í™˜ë³¸]

ëª©í‘œ: Streamlit + OpenAI(ChatOpenAI) ê¸°ë°˜ RAG ì±—ë´‡ì„
â†’ **ë¡œì»¬ ì˜¤í”ˆì†ŒìŠ¤ LLM (Ollamaì˜ DeepSeek-R1)** + **Gradio UI**ë¡œ ë³€í™˜

í•µì‹¬ ë³€ê²½ì 
1) OpenAI/í‚¤ ì œê±°, **Ollama ë¡œì»¬ ì„œë²„(http://localhost:11434)** ì‚¬ìš©
2) ëª¨ë¸: deepseek-r1 (ì˜ˆ: "deepseek-r1:latest" ë˜ëŠ” "deepseek-r1:7b/14b")
3) Streamlit â†’ **Gradio ChatInterface** (ìŠ¤íŠ¸ë¦¬ë° ëŒ€ì‘)
4) ê¸°ì¡´ `retriever` ëª¨ë“ˆì˜ ì²´ì¸(`query_augmentation_chain`, `retriever`, `document_chain`)ì€ **ê·¸ëŒ€ë¡œ** ì‚¬ìš©
5) DeepSeek-R1ì˜ ë‚´ë¶€ ì‚¬ê³ (<think>...</think>)ëŠ” ì‚¬ìš©ì ì¶œë ¥ì—ì„œ **ìë™ìœ¼ë¡œ ìˆ¨ê¹€**

ì‚¬ì „ ì¤€ë¹„
- Ollama ì„¤ì¹˜: https://ollama.com  í›„ `ollama pull deepseek-r1:latest`
- íŒ¨í‚¤ì§€: `pip install gradio langchain langchain-community`
- (ì¤‘ìš”) `retriever.py`ê°€ ë™ì¼ ê²½ë¡œì— ì¡´ì¬í•˜ê³ , ì•„ë˜ ì†ì„±ì„ ì œê³µí•œë‹¤ê³  ê°€ì •:
  - retriever.query_augmentation_chain
  - retriever.retriever
  - retriever.document_chain (stream ë©”ì„œë“œ ì§€ì›)

ì‹¤í–‰
- `python app.py` ì‹¤í–‰ â†’ ë¸Œë¼ìš°ì €ì—ì„œ http://127.0.0.1:7860
"""

from __future__ import annotations
from typing import List, Tuple, Generator
import re

import gradio as gr

# LangChain ë©”ì‹œì§€ íƒ€ì… (ì› ì½”ë“œì™€ ë™ì¼ ì¸í„°í˜ì´ìŠ¤)
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

# LangChainìš© Ollama ì±„íŒ… ëª¨ë¸ (ë¡œì»¬ LLM)
from langchain_community.chat_models import ChatOllama

# ê¸°ì¡´ RAG êµ¬ì„±ìš”ì†Œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
import retriever


# =========================
# ì„¤ì •
# =========================
MODEL_NAME = "deepseek-r1:latest"   # í•„ìš” ì‹œ "deepseek-r1:7b"/"deepseek-r1:14b"
TEMPERATURE = 0.7
SYSTEM_PROMPT = "ë„ˆëŠ” ë¬¸ì„œì— ê¸°ë°˜í•´ ë‹µë³€í•˜ëŠ” ë„ì‹œ ì •ì±… ì „ë¬¸ê°€ì•¼"

# DeepSeek-R1ì˜ ë‚´ë¶€ ì‚¬ê³ (<think>...</think>) ì œê±°ìš© ì •ê·œì‹
THINK_TAG_RE = re.compile(r"<think>.*?</think>", re.DOTALL)


def strip_think(text: str) -> str:
    """
    ìµœì¢… í…ìŠ¤íŠ¸ì—ì„œ <think>...</think> ë‚´ë¶€ ë‚´ìš©ì„ ì œê±°.
    """
    return THINK_TAG_RE.sub("", text).strip()


def stream_hide_think(chunks: Generator[str, None, None]) -> Generator[str, None, None]:
    """
    ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì—ë„ <think> ì„¹ì…˜ì„ í™”ë©´ì— ë…¸ì¶œí•˜ì§€ ì•Šë„ë¡ ì œê±°.
    ê°„ë‹¨í•œ ìƒíƒœë¨¸ì‹ ìœ¼ë¡œ '<think>' ~ '</think>' ì‚¬ì´ í…ìŠ¤íŠ¸ë¥¼ í•„í„°ë§.
    """
    in_think = False
    buf = ""

    for piece in chunks:
        s = str(piece)
        buf += s
        out_parts = []
        i = 0

        while i < len(buf):
            if not in_think:
                start = buf.find("<think>", i)
                if start == -1:
                    out_parts.append(buf[i:])
                    buf = ""
                    break
                else:
                    out_parts.append(buf[i:start])
                    i = start + len("<think>")
                    in_think = True
            else:
                end = buf.find("</think>", i)
                if end == -1:
                    # ì¢…ë£Œ íƒœê·¸ê°€ ì•„ì§ ì•ˆ ì™”ìœ¼ë©´ ë‹¤ìŒ piece ê¸°ë‹¤ë¦¼
                    buf = buf[i:]
                    i = len(buf)
                    break
                else:
                    # think ë¸”ë¡ì„ ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰
                    i = end + len("</think>")
                    in_think = False

        visible = "".join(out_parts)
        if visible:
            yield visible


# =========================
# ë¡œì»¬ LLM (Ollama) ì¸ìŠ¤í„´ìŠ¤
# =========================
# ì£¼ì˜: Ollama ì„œë²„(ê¸°ë³¸: http://localhost:11434)ê°€ ë–  ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
llm = ChatOllama(
    model=MODEL_NAME,
    temperature=TEMPERATURE,
    # base_url="http://localhost:11434",  # ê¸°ë³¸ê°’ì´ë©´ ìƒëµ ê°€ëŠ¥
)


# =========================
# Gradio <-> LangChain íˆìŠ¤í† ë¦¬ ë³€í™˜
# =========================
def history_to_messages(history: List[Tuple[str, str]]) -> List:
    """
    Gradio ChatInterfaceì˜ history([[user, assistant], ...])ë¥¼
    LangChain ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜. í•­ìƒ SYSTEM_PROMPTë¥¼ ì²« ë©”ì‹œì§€ë¡œ í¬í•¨.
    """
    msgs: List = [SystemMessage(SYSTEM_PROMPT)]
    for user_text, ai_text in history:
        if user_text:
            msgs.append(HumanMessage(user_text))
        if ai_text:
            msgs.append(AIMessage(ai_text))
    return msgs


# =========================
# ë©”ì¸ ì‘ë‹µ ë¡œì§
# =========================
def respond(user_input: str, history: List[Tuple[str, str]]):
    """
    ChatInterface ì½œë°±:
      1) ê¸°ì¡´ íˆìŠ¤í† ë¦¬ + ì‚¬ìš©ì ì…ë ¥ êµ¬ì„±
      2) ì§ˆì˜ í™•ì¥(query_augmentation_chain)
      3) ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰(retriever.invoke)
      4) ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¡œ document_chain.stream ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
      5) ìŠ¤íŠ¸ë¦¬ë° ë™ì•ˆ <think> ì„¹ì…˜ ì œê±°

    ë°˜í™˜ í˜•ì‹:
      - yield (partial_text, side_info_markdown)
    """
    # 1) ë©”ì‹œì§€ êµ¬ì„±
    messages = history_to_messages(history) + [HumanMessage(user_input)]

    # 2) ì§ˆì˜ í™•ì¥
    try:
        augmented_query = retriever.query_augmentation_chain.invoke({
            "messages": messages,
            "query": user_input,
        })
    except Exception as e:
        augmented_query = ""
        # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

    # 3) ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    try:
        docs = retriever.retriever.invoke(f"{user_input}\n{augmented_query}")
    except Exception:
        docs = []

    # ìš°ì¸¡ íŒ¨ë„ì— í‘œì‹œí•  ê°„ë‹¨ ì •ë³´(ë¬¸ì„œ ìˆ˜/ë³´ê°• ì§ˆì˜)
    side_info = [
        "### ğŸ” RAG ì»¨í…ìŠ¤íŠ¸",
        f"- ê´€ë ¨ ë¬¸ì„œ ìˆ˜: **{len(docs)}**",
        f"- ë³´ê°• ì§ˆì˜(Augmented Query):",
        f"```\n{augmented_query}\n```" if augmented_query else "_ìƒì„± ì‹¤íŒ¨_",
    ]
    side_md = "\n".join(side_info)

    # 4) ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¡œ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
    try:
        raw_stream = retriever.document_chain.stream({
            "messages": messages,
            "context": docs,
        })
    except Exception as e:
        yield f"ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆì–´ìš”.\nì‚¬ìœ : {e}", side_md
        return

    # 5) ë‚´ë¶€ ì‚¬ê³ (<think>) ìˆ¨ê¹€
    safe_stream = stream_hide_think(raw_stream)

    for chunk in safe_stream:
        yield chunk, side_md


# =========================
# Gradio UI êµ¬ì„±
# =========================
with gr.Blocks(css="footer {visibility: hidden}") as demo:
    gr.Markdown("## ğŸ™ï¸ ë¬¸ì„œê¸°ë°˜ ë„ì‹œì •ì±… ì±—ë´‡ (DeepSeek-R1 on Ollama + Gradio)")

    chat = gr.ChatInterface(
        fn=respond,
        additional_outputs=[gr.Markdown(label="ì„¸ë¶€ ì •ë³´")],
        chatbot=gr.Chatbot(
            label="ë„ì‹œì •ì±… ì „ë¬¸ê°€",
            height=520,
        ),
        title="",
        description=(
            "ë¡œì»¬ **Ollama**ì˜ **DeepSeek-R1** ëª¨ë¸ê³¼ ê¸°ì¡´ `retriever` ì²´ì¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"
            "â€» ëª¨ë¸ì˜ ë‚´ë¶€ ì‚¬ê³ (<think>...</think>)ëŠ” ìë™ìœ¼ë¡œ ìˆ¨ê²¨ì§‘ë‹ˆë‹¤."
        ),
        theme="soft",
        retry_btn="ë‹¤ì‹œ ìƒì„±",
        undo_btn="ì´ì „ ë©”ì‹œì§€ ì‚­ì œ",
        clear_btn="ëŒ€í™” ì´ˆê¸°í™”",
        examples=[
            "ë„ì‹œ ì—´ì„¬ ì™„í™”ë¥¼ ìœ„í•œ ë‹¨ê¸° ì •ì±…ì€?",
            "ë³´í–‰ì¹œí™”êµ¬ì—­ ì§€ì • ì‹œ ê³ ë ¤í•´ì•¼ í•  ì§€í‘œëŠ”?",
            "ìš°ë¦¬ êµ¬ì˜ ì£¼ì°¨ ìˆ˜ìš” ê´€ë¦¬ ë°©ì•ˆ ì‚¬ë¡€ê°€ ìˆì„ê¹Œ?",
        ],
    )

    with gr.Accordion("ì‹œìŠ¤í…œ/ëª¨ë¸ ì •ë³´", open=False):
        gr.Markdown(
            f"""
- ì‚¬ìš© ëª¨ë¸: **{MODEL_NAME}**  
- ë¡œì»¬ ì„œë²„: **http://localhost:11434** (Ollama ê¸°ë³¸)  
- ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: `{SYSTEM_PROMPT}`  
- Temperature: `{TEMPERATURE}`  
- ì‚¬ìš© ì²´ì¸: `retriever.query_augmentation_chain`, `retriever.retriever`, `retriever.document_chain`
"""
        )

if __name__ == "__main__":
    # ë‚´ë¶€ë§ë§Œ ì‚¬ìš©í•  ê²½ìš° share=False ê¶Œì¥
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)
