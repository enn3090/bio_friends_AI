"""
[OpenCode ë³€í™˜ë³¸]

ëª©í‘œ: LangGraph + ChatOpenAI(Closed LLM) ì½˜ì†” ì½”ë“œë¥¼
â†’ **ë¡œì»¬ ì˜¤í”ˆì†ŒìŠ¤ LLM(Ollama Â· DeepSeek-R1)** + **Gradio UI**ë¡œ ë³€í™˜

í•µì‹¬ ë³€ê²½ì 
1) ChatOpenAI â†’ ChatOllama (API í‚¤ ë¶ˆí•„ìš”, ë¡œì»¬ http://localhost:11434)
2) ì½˜ì†” while-loop â†’ Gradio ChatInterface
3) LangGraph(StateGraph, MemorySaver) ìœ ì§€: ëŒ€í™” ìƒíƒœ/ì²´í¬í¬ì¸íŠ¸ ê·¸ëŒ€ë¡œ í™œìš©
4) DeepSeek-R1ì´ ìƒì„±í•˜ëŠ” ë‚´ë¶€ ì‚¬ê³ (<think>...</think>)ëŠ” ì‚¬ìš©ì ì¶œë ¥ì—ì„œ ìë™ ì œê±°
5) íƒ€ì… ì•ˆì „: LangChain ë©”ì‹œì§€ íƒ€ì…(list[BaseMessage])ë¡œ ìƒíƒœ ì •ì˜

ì‚¬ì „ ì¤€ë¹„
- Ollama ì„¤ì¹˜: https://ollama.com  í›„ ëª¨ë¸ ë°›ê¸°
  `ollama pull deepseek-r1:latest`
- íŒŒì´ì¬ íŒ¨í‚¤ì§€:
  `pip install gradio langchain langchain-community langgraph`

ì‹¤í–‰
- `python app.py` ì‹¤í–‰ í›„ ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸
"""

from __future__ import annotations
import re
import uuid
from typing import Annotated, List, TypedDict

import gradio as gr

# LangChain ë©”ì‹œì§€ íƒ€ì…
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
# LangChain Ollama ì±— ëª¨ë¸ (ë¡œì»¬ LLM)
from langchain_community.chat_models import ChatOllama

# LangGraph (ìƒíƒœ ê·¸ë˜í”„, ì²´í¬í¬ì¸íŠ¸)
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver


# ===== ì„¤ì • =====
MODEL_NAME = "deepseek-r1:latest"      # í•„ìš” ì‹œ "deepseek-r1:7b", "deepseek-r1:14b" ë“±
TEMPERATURE = 0.7
SYSTEM_PROMPT = "ë„ˆëŠ” ì‚¬ìš©ìë¥¼ ë„ì™€ì£¼ëŠ” ìƒë‹´ì‚¬ì•¼."

# DeepSeek-R1 ë‚´ë¶€ ì‚¬ê³ (<think>...</think>) ì œê±°ìš©
THINK_TAG_RE = re.compile(r"<think>.*?</think>", re.DOTALL)


def strip_think(text: str) -> str:
    """DeepSeek-R1ì´ ì¶œë ¥í•˜ëŠ” ë‚´ë¶€ ì‚¬ê³  ë¸”ë¡ì„ ì œê±°."""
    return THINK_TAG_RE.sub("", text).strip()


# ===== LangGraph ìƒíƒœ ì •ì˜ =====
class State(TypedDict):
    """
    LangGraphì—ì„œ ì‚¬ìš©í•  ìƒíƒœ ìŠ¤í‚¤ë§ˆ.

    - messages: ëª¨ë“  ëŒ€í™” ë©”ì‹œì§€ ëª©ë¡(BaseMessage). add_messagesë¥¼ ì‚¬ìš©í•˜ë©´
      ìƒˆ ë©”ì‹œì§€ë¥¼ ë®ì–´ì“°ì§€ ì•Šê³  ë¦¬ìŠ¤íŠ¸ì— ìë™ìœ¼ë¡œ ëˆ„ì ëœë‹¤.
    """
    messages: Annotated[List[BaseMessage], add_messages]


# ===== ë¡œì»¬ LLM (Ollama) =====
llm = ChatOllama(
    model=MODEL_NAME,
    temperature=TEMPERATURE,
    # base_url="http://localhost:11434",  # ê¸°ë³¸ê°’ì´ë©´ ìƒëµ ê°€ëŠ¥
)


# ===== ê·¸ë˜í”„ ë…¸ë“œ: ëª¨ë¸ ì‘ë‹µ ìƒì„± =====
def generate(state: State) -> dict:
    """
    í˜„ì¬ê¹Œì§€ì˜ state["messages"]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ì— ì§ˆì˜í•˜ê³ ,
    ëª¨ë¸ì´ ìƒì„±í•œ AIMessageë¥¼ messagesì— ì¶”ê°€í•˜ì—¬ ë°˜í™˜í•œë‹¤.
    """
    # llm.invokeëŠ” AIMessageë¥¼ ë°˜í™˜
    ai_msg = llm.invoke(state["messages"])
    # DeepSeekì˜ <think> ì œê±°
    cleaned = strip_think(ai_msg.content)
    ai_msg = AIMessage(cleaned)
    return {"messages": [ai_msg]}


# ===== ê·¸ë˜í”„ êµ¬ì„± =====
graph_builder = StateGraph(State)
graph_builder.add_node("generate", generate)
graph_builder.add_edge(START, "generate")
graph_builder.add_edge("generate", END)

# ì²´í¬í¬ì¸íŠ¸(ë©”ëª¨ë¦¬) ì„¤ì •
memory = MemorySaver()
graph = graph_builder.compile(checkpointer=memory)


# ===== Gradio â†” LangGraph ë¸Œë¦¿ì§€ ìœ í‹¸ =====
def ensure_thread_id(thread_id_state: str | None) -> str:
    """ì„¸ì…˜ë³„ thread_idë¥¼ ìƒì„±/ìœ ì§€."""
    return thread_id_state or str(uuid.uuid4())


def bootstrap_if_needed(initialized: bool, thread_id: str):
    """
    ìµœì´ˆ 1íšŒì— í•œí•´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ìŠ¤ë ˆë“œì— ì£¼ì….
    LangGraphëŠ” checkpointerë¥¼ ì“°ë¯€ë¡œ ê°™ì€ thread_idë¡œ í˜¸ì¶œí•˜ë©´ ì´ì „ ìƒíƒœë¥¼ ê¸°ì–µí•œë‹¤.
    """
    if initialized:
        return True  # ì´ë¯¸ ì´ˆê¸°í™”ë¨

    # START ë‹¨ê³„ì—ì„œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ë¨¼ì € ë„£ê³  generateë¥¼ ì‹¤í–‰í•˜ë„ë¡
    # graph.stream í˜¸ì¶œë¡œ ë¶€íŠ¸ìŠ¤íŠ¸ë©í•œë‹¤.
    init_state = {
        "messages": [SystemMessage(SYSTEM_PROMPT)]
    }
    # í•œ ë²ˆ ëŒë ¤ ìƒíƒœë¥¼ ì €ì¥(ë©”ëª¨ë¦¬ì— checkpointer)
    for _ in graph.stream(init_state, {"configurable": {"thread_id": thread_id}}, stream_mode="values"):
        # ìƒì„±ëœ ë©”ì‹œì§€ëŠ” UIì— ë°”ë¡œ ë³´ì—¬ì¤„ í•„ìš” ì—†ìŒ(ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë§Œ ë„£ì—ˆê¸° ë•Œë¬¸)
        pass
    return True


# ====== Gradio ì½œë°± ======
def respond(user_input: str, history: List[tuple[str, str]], thread_id_state: str, inited: bool):
    """
    Gradio ChatInterface ì½œë°±.
    - ì„¸ì…˜ thread_id í™•ë³´ â†’ LangGraph ì²´í¬í¬ì¸íŠ¸ì™€ ì—°ê²°
    - ìµœì´ˆ ìš”ì²­ì´ë©´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¡œ ë¶€íŠ¸ìŠ¤íŠ¸ë©
    - ì‚¬ìš©ì ì…ë ¥ì„ ê·¸ë˜í”„ì— í˜ë ¤ë³´ë‚´ê³ , ë…¸ë“œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìˆœì°¨ ìŠ¤íŠ¸ë¦¬ë°
    """
    thread_id = ensure_thread_id(thread_id_state)
    inited = bootstrap_if_needed(inited, thread_id)

    # LangGraph ì‹¤í–‰: HumanMessageë¥¼ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
    # stream_mode="values"ë¡œ ê° ë…¸ë“œ ì‹¤í–‰ í›„ì˜ ì „ì²´ ìƒíƒœë¥¼ ë°›ëŠ”ë‹¤.
    full_text = ""
    for event in graph.stream(
        {"messages": [HumanMessage(user_input)]},
        {"configurable": {"thread_id": thread_id}},
        stream_mode="values",
    ):
        # event["messages"]ëŠ” ì§€ê¸ˆê¹Œì§€ ëˆ„ì ëœ ëª¨ë“  ë©”ì‹œì§€
        last = event["messages"][-1]
        if isinstance(last, AIMessage):
            # ì´ë²ˆ í„´ì— ìƒì„±ëœ ìµœì‹  AI ì‘ë‹µë§Œ ìŠ¤íŠ¸ë¦¬ë°
            chunk = last.content
            full_text = chunk
            yield chunk, thread_id, True  # (ì‘ë‹µ í…ìŠ¤íŠ¸, thread_id ìœ ì§€, ì´ˆê¸°í™” ì—¬ë¶€ True)

    # ë§ˆì§€ë§‰ í•œ ë²ˆ ë” ë°˜í™˜í•  í•„ìš”ëŠ” ì—†ì§€ë§Œ, ì•ˆì „í•˜ê²Œ ìµœì¢…ê°’ì„ ì¬ì „ì†¡í•˜ì§€ ì•Šê³  ì¢…ë£Œ
    return


# ====== Gradio UI ======
with gr.Blocks(css="footer {visibility: hidden}") as demo:
    gr.Markdown("## ğŸ’¬ LangGraph Chat (DeepSeek-R1 on Ollama + Gradio)")

    # ì„¸ì…˜ ìƒíƒœ: thread_id ë° ì´ˆê¸°í™” ì—¬ë¶€
    thread_id_state = gr.State("")
    initialized = gr.State(False)

    chat = gr.ChatInterface(
        fn=respond,
        additional_inputs=[thread_id_state, initialized],
        additional_outputs=[thread_id_state, initialized],
        chatbot=gr.Chatbot(
            label="ìƒë‹´ì‚¬",
            height=520,
        ),
        title="",
        description=(
            "LangGraph + ë¡œì»¬ **Ollama(DeepSeek-R1)** ê¸°ë°˜ ì±—ë´‡ì…ë‹ˆë‹¤.\n"
            "ì„¸ì…˜ë³„ë¡œ LangGraphì˜ ë©”ëª¨ë¦¬(MemorySaver)ë¥¼ ì‚¬ìš©í•´ ëŒ€í™” ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\n"
            "â€» ëª¨ë¸ì˜ ë‚´ë¶€ ì‚¬ê³ (<think>...</think>)ëŠ” ìë™ìœ¼ë¡œ ìˆ¨ê¹ë‹ˆë‹¤."
        ),
        theme="soft",
        retry_btn="ë‹¤ì‹œ ìƒì„±",
        undo_btn="ì´ì „ ë©”ì‹œì§€ ì‚­ì œ",
        clear_btn="ëŒ€í™” ì´ˆê¸°í™”",
        examples=[
            "ìš”ì¦˜ ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ë§ì•„ìš”. ì–´ë–»ê²Œ ê´€ë¦¬í•˜ë©´ ì¢‹ì„ê¹Œìš”?",
            "ì—…ë¬´ ëª°ì…ë„ë¥¼ ë†’ì´ëŠ” ë° ë„ì›€ ë˜ëŠ” ë£¨í‹´ì„ ì¶”ì²œí•´ì¤˜.",
            "íšŒì˜ì—ì„œ ì˜ê²¬ì„ ì„¤ë“ë ¥ ìˆê²Œ ì „ë‹¬í•˜ëŠ” íŒì´ ìˆì„ê¹Œ?",
        ],
    )

    with gr.Accordion("ì‹œìŠ¤í…œ/ëª¨ë¸ ì •ë³´", open=False):
        gr.Markdown(
            f"""
- ì‚¬ìš© ëª¨ë¸: **{MODEL_NAME}**  
- Temperature: **{TEMPERATURE}**  
- ë¡œì»¬ ì„œë²„: **http://localhost:11434** (Ollama ê¸°ë³¸)  
- ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: `{SYSTEM_PROMPT}`
- LangGraph: `StateGraph`, `MemorySaver` ì‚¬ìš© (ì„¸ì…˜ë³„ thread_idë¡œ ìƒíƒœ ìœ ì§€)
"""
        )

if __name__ == "__main__":
    # ë‚´ë¶€ë§ë§Œ ì‚¬ìš©í•  ê²½ìš° share=False ê¶Œì¥
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)


